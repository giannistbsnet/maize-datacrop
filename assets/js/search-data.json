{"0": {
    "doc": "Maize Setup",
    "title": "DataCROP Maize Workflow Management Engine Deployment",
    "content": "You can set up the Maize stack in two ways: . | Maize MVP (recommended) — a single repository and script that brings up all Maize components together. Start here if you want the fastest path to a working demo. | Manual per-repository setup — deploy each component yourself (Keycloak, Airflow, Worker, Model Repository, Workflow Editor) for fine-grained control. | . Use the MVP path if you want a quick, reproducible deployment. Choose the manual path if you need to customize each service, change infrastructure details, or operate components independently. ",
    "url": "/Setup/#datacrop-maize-workflow-management-engine-deployment",
    
    "relUrl": "/Setup/#datacrop-maize-workflow-management-engine-deployment"
  },"1": {
    "doc": "Maize Setup",
    "title": "Option 1: Maize MVP (single script)",
    "content": "Follow the instructions in the Maize MVP Setup page to configure the env files in each component folder of the MVP repository and run the one-shot setup script. ",
    "url": "/Setup/#option-1-maize-mvp-single-script",
    
    "relUrl": "/Setup/#option-1-maize-mvp-single-script"
  },"2": {
    "doc": "Maize Setup",
    "title": "Option 2: Manual per-repository setup",
    "content": "If you prefer to deploy each component separately, go to the Manual Setup landing page and follow the component pages in order: . | Keycloak Setup | Airflow Setup | Worker Setup | Model Repository Setup | Workflow Editor Setup | . You can still reference the manual pages even when using the MVP path to understand what each service does and what each environment variable controls. ",
    "url": "/Setup/#option-2-manual-per-repository-setup",
    
    "relUrl": "/Setup/#option-2-manual-per-repository-setup"
  },"3": {
    "doc": "Maize Setup",
    "title": "Maize Setup",
    "content": " ",
    "url": "/Setup/",
    
    "relUrl": "/Setup/"
  },"4": {
    "doc": "1. Maize MVP Setup",
    "title": "Maize MVP (single-script deployment)",
    "content": "Use the Maize MVP repository to bring up all Maize components with one script. This path is the fastest way to get a working demo; it provisions Keycloak automatically as part of the run. ",
    "url": "/maize-mvp/#maize-mvp-single-script-deployment",
    
    "relUrl": "/maize-mvp/#maize-mvp-single-script-deployment"
  },"5": {
    "doc": "1. Maize MVP Setup",
    "title": "Repository",
    "content": ". | Maize MVP repo: https://github.com/datacrop/maize-mvp (use your fork/URL if different). | . ",
    "url": "/maize-mvp/#repository",
    
    "relUrl": "/maize-mvp/#repository"
  },"6": {
    "doc": "1. Maize MVP Setup",
    "title": "Prerequisites",
    "content": ". | Docker installed on the host. | Network/ports open for Keycloak, Airflow, Worker, Model Repository, and Workflow Editor. | . ",
    "url": "/maize-mvp/#prerequisites",
    
    "relUrl": "/maize-mvp/#prerequisites"
  },"7": {
    "doc": "1. Maize MVP Setup",
    "title": "Configure environment variables",
    "content": ". | Clone the MVP repository and cd into it. | For each component subfolder (Keycloak, Airflow, Worker, Model Repository, Workflow Editor, etc.), open its .env or config file and adjust values for your environment (IP/hostnames, ports, credentials where applicable). | Use the corresponding manual pages for variable meanings if needed: . | Keycloak, Airflow, Worker, Model Repository, Workflow Editor | . | . ",
    "url": "/maize-mvp/#configure-environment-variables",
    
    "relUrl": "/maize-mvp/#configure-environment-variables"
  },"8": {
    "doc": "1. Maize MVP Setup",
    "title": "Run the setup script",
    "content": ". | In the repo root, run the provided setup script (typically ./scripts/setup.sh; mark executable first if needed: chmod +x scripts/setup.sh). | The script will run docker compose up for all components. Wait until containers are healthy. | . ",
    "url": "/maize-mvp/#run-the-setup-script",
    
    "relUrl": "/maize-mvp/#run-the-setup-script"
  },"9": {
    "doc": "1. Maize MVP Setup",
    "title": "Verify the deployment",
    "content": ". | Check containers: . | docker ps --filter name=keycloak | docker ps --filter name=airflow | docker ps --filter name=worker | docker ps --filter name=wme (model repo) | docker ps --filter name=wme-ui (workflow editor) | . | Open the Workflow Editor UI at http://&lt;WME_UI_HOST&gt;:5173/MainPage/Warehouse and confirm it loads. | . ",
    "url": "/maize-mvp/#verify-the-deployment",
    
    "relUrl": "/maize-mvp/#verify-the-deployment"
  },"10": {
    "doc": "1. Maize MVP Setup",
    "title": "Post-deployment (required)",
    "content": "After the Workflow Editor UI is reachable: . | Log in via Keycloak. | Go to Settings in the editor UI. | Click Initialize resources (see Workflow Editor Setup for details). | This creates the base resources and any extra processors defined via the Model Repository. Skipping this leaves the system uninitialized. | . ",
    "url": "/maize-mvp/#post-deployment-required",
    
    "relUrl": "/maize-mvp/#post-deployment-required"
  },"11": {
    "doc": "1. Maize MVP Setup",
    "title": "Notes on Keycloak",
    "content": ". | In the MVP path, Keycloak is provisioned/configured automatically by the script; you do not need a pre-existing Keycloak instance. | If you need to customize Keycloak manually, refer to the Keycloak Setup page. | . ",
    "url": "/maize-mvp/#notes-on-keycloak",
    
    "relUrl": "/maize-mvp/#notes-on-keycloak"
  },"12": {
    "doc": "1. Maize MVP Setup",
    "title": "1. Maize MVP Setup",
    "content": " ",
    "url": "/maize-mvp/",
    
    "relUrl": "/maize-mvp/"
  },"13": {
    "doc": "2. Airflow Setup",
    "title": "DataCROP Maize Airflow Processing Engine Deployment",
    "content": "Use this page when following the manual per-repository setup. If you use Maize MVP, this component is deployed by the MVP script; refer here only for customization or troubleshooting. See Maize Setup for the two setup options. This is a demo deployment instance for the Maize DataCROP version. It deploys the Airflow web server responsible for managing tasks within the DataCROP Workflow Management Engine infrastructure. The deployment consists of six containers. ",
    "url": "/airflow/#datacrop-maize-airflow-processing-engine-deployment",
    
    "relUrl": "/airflow/#datacrop-maize-airflow-processing-engine-deployment"
  },"14": {
    "doc": "2. Airflow Setup",
    "title": "Overview",
    "content": "The DataCROP Maize Airflow Processing Engine is a critical component of the DataCROP Workflow Management Engine. This engine is responsible for orchestrating and managing the execution of various tasks (DAGs) within the DataCROP infrastructure, providing an interface to monitor and manage workflows through the Airflow webserver. ",
    "url": "/airflow/#overview",
    
    "relUrl": "/airflow/#overview"
  },"15": {
    "doc": "2. Airflow Setup",
    "title": "Requirements",
    "content": ". | Docker-CE | . ",
    "url": "/airflow/#requirements",
    
    "relUrl": "/airflow/#requirements"
  },"16": {
    "doc": "2. Airflow Setup",
    "title": "Prerequisites",
    "content": "Before proceeding with the deployment, make sure to complete the following steps: . After completing the setup, follow these steps to configure your environment variables: . | Navigate to the .env file and ensure that all necessary environment variables are set correctly for your deployment. Current values from maze-processing-engine-airflow/.env are shown below; sensitive secrets are redacted—keep using the real values already present in your .env. # AIRFLOW USERS || DC.C AIRFLOW_UID=1002 DOCKER_GID=988 AIRFLOW_WEB_PORT=8080 AIRFLOW_WWW_UNAME_USERNAME='airflow' AIRFLOW_WEB_PSSWD=[REDACTED – keep existing value in your .env] AIRFLOW_WEB_SSL_CERT=/security/airflow/airflow.pem AIRFLOW_WEB_SSL_KEY=/security/airflow/airflow-key.pem AIRFLOW_CA_CERT=/security/ca/rootCA.pem AIRFLOW_WEB_SECRET_KEY=[REDACTED – keep existing value in your .env] AIRFLOW_FERNET_KEY=[REDACTED – keep existing value in your .env] # HOST || DC.C HOST_IP=&lt;HOST_IP&gt; # WORKER_NAME=worker01 # REDIS || DC.C REDIS_TLS_PORT=6379 REDIS_TLS_CERT_FILE=/security/redis/redis.pem REDIS_TLS_KEY_FILE=/security/redis/redis-key.pem REDIS_TLS_CA_CERT_FILE=/security/ca/rootCA.pem REDIS_TLS_CLIENT_CERT_FILE=/security/redis/redis-client.pem REDIS_TLS_CLIENT_KEY_FILE=/security/redis/redis-client-key.pem # POSTGRES || DC.C POSTGRES_PORT=5432 POSTGRES_SSL_CERT_FILE=/security/postgres/postgres.pem POSTGRES_SSL_KEY_FILE=/security/postgres/postgres-key.pem POSTGRES_SSL_CA_FILE=/security/ca/rootCA.pem # Celery || DC.C CELERY_WEB_UNAME=[REDACTED – keep existing value in your .env] CELERY_WEB_PSSWD=[REDACTED – keep existing value in your .env] CELERY_FLOWER_PORT=5555 CELERY_FLOWER_CERT=/security/flower/flower.pem CELERY_FLOWER_KEY=/security/flower/flower-key.pem CELERY_FLOWER_CA_CERT=/security/ca/rootCA.pem REMOTE_WORKER_NAME=&lt;REMOTE_WORKER_NAME&gt; REMOTE_WORKER_IP=&lt;REMOTE_WORKER_IP&gt; . Sensitive secrets are redacted above; ensure your .env retains the real values currently configured. | . Once these parameters are correctly set, you can proceed with the deployment. Start The Application. | Navigate to the source directory containing the docker-compose.yml file. | Run the following command: . docker compose up -d . | . Verify that everything is up and running . Wait for the services to start, then run the following commands: . | Check if the container is running (change worker_name with the actual name that you specified in the .env file): . docker ps --filter name=airflow --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES airflow-airflow-triggerer airflow-airflow-triggerer-1 airflow-airflow-webserver airflow-airflow-webserver-1 airflow-flower airflow-flower-1 airflow-airflow-scheduler airflow-airflow-scheduler-1 postgres:13 airflow-postgres-1 redis:7.2 airflow-redis-1 . | . Make Sure Everything Works . | Access the Flower Web App: . | Open a browser and navigate to http://{Your IP}:5555/workers. | Log in using the celery credentials you provided in the .env file. | After successful authentication, you should be redirected to the workers page, confirming that Celery was set up correctly. | . | Access the Airflow Web App: . | Open a browser and go to http://{Your IP}:8080/home. | Log in using the credentials provided by your organization for Airflow. | You should see all the available DAGs listed, confirming that your DAGs folder is properly configured. | . | . Stop everything. Navigate to the source directory and run the following command. docker-compose down . ",
    "url": "/airflow/#prerequisites",
    
    "relUrl": "/airflow/#prerequisites"
  },"17": {
    "doc": "2. Airflow Setup",
    "title": "2. Airflow Setup",
    "content": " ",
    "url": "/airflow/",
    
    "relUrl": "/airflow/"
  },"18": {
    "doc": "5. Workflow Editor Setup",
    "title": "DataCROP Maize Workflow Management Editor Deployment",
    "content": "This is a demo deployment instance for the Maize DataCROP version. It deploys the DataCROP Workflow Management Engine web application for creating and managing workflows. The deployment consists of a single container. Use this page when following the manual per-repository setup. If you use Maize MVP, the editor is deployed by the MVP script; follow the initialization steps below once it is up. See Maize Setup for the two setup options. Prerequisites . Before proceeding, ensure you have completed the setup instructions for the Maize Model Repository. After completing the setup, follow these steps to configure your environment variables: . | Navigate to the .env file in the ui directory. | Update the file with the correct values for your infrastructure. Current values from maze-workflow-management-editor/.env are: . VITE_API_URL=http://&lt;WME_API_HOST&gt;:9090 VITE_BACKEND_IP=&lt;WME_API_HOST&gt; VITE_AIRFLOW_IP=&lt;AIRFLOW_HOST&gt; VITE_KEYCLOAK_URL=https://&lt;KEYCLOAK_HOST&gt;/ VITE_KEYCLOAK_REALM=&lt;KEYCLOAK_REALM&gt; VITE_KEYCLOAK_CLIENT_ID=&lt;KEYCLOAK_CLIENT_ID&gt; VITE_PROJECT_NAME=&lt;PROJECT_NAME&gt; VITE_DEFAULT_PRIMARY_COLOR=#DA3333 . Adjust the above values only if your deployment differs (e.g., different host IP or Keycloak realm). No secrets are stored in this file. | . Once these parameters are correctly set, you can proceed with the deployment. REQUIREMENTS . | Docker-CE | . Start The Application. | Clone the repository and navigate to the source directory containing the docker-compose.yml file. | Run the following command: . docker compose up -d . | . Verify that everything is up and running . Wait for the services to start, then run the following commands: . | Check if the container is running: . docker ps --filter name=wme-ui --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES node:lts-alpine wme-ui . | Check if the network is set up correctly: . docker network ls --filter name=\"maze-workflow-management-editor_wme-network\" --format \"table {{.Name}}\\t{{.Driver}}\\t{{.Scope}}\" . Ensure the output matches the following: . NAME DRIVER SCOPE maze-workflow-management-editor_wme-network bridge local . | . Post-deployment initialization (required) . Perform this step after the UI is reachable (applies to both Maize MVP and manual setups): . | Log in through Keycloak. | In the Workflow Editor UI, open Settings. | Click Initialize resources. | . What this does: . | Creates the base data models, workers, digital resources, and processor definitions needed for the application to function. | Imports any custom processors defined in config/extra-processors.json of the Model Repository (see Model Repository Setup). | . Run this once per environment after deployments. Skipping it leaves the system uninitialized. Make Sure Everything Works . | Open a browser and go to http://&lt;WME_UI_HOST&gt;:5173/MainPage/Warehouse. | You will be redirected to the Keycloak authentication page. Enter the credentials provided by your organization. | After successful authentication, you will be redirected to the main page of the workflow management engine, where you can begin creating and managing workflows. | . Stop everything. Navigate to the source directory and run the following command. docker-compose down . ",
    "url": "/editor/#datacrop-maize-workflow-management-editor-deployment",
    
    "relUrl": "/editor/#datacrop-maize-workflow-management-editor-deployment"
  },"19": {
    "doc": "5. Workflow Editor Setup",
    "title": "5. Workflow Editor Setup",
    "content": " ",
    "url": "/editor/",
    
    "relUrl": "/editor/"
  },"20": {
    "doc": "2. Manual Setup",
    "title": "Manual per-repository setup",
    "content": "Use this path if you want to deploy and customize each component separately. Follow the component pages in order: . | Keycloak Setup | Airflow Setup | Worker Setup | Model Repository Setup | Workflow Editor Setup | . You can still reference these pages even if you use the Maize MVP path, to understand variables and configuration details. ",
    "url": "/manual-setup/#manual-per-repository-setup",
    
    "relUrl": "/manual-setup/#manual-per-repository-setup"
  },"21": {
    "doc": "2. Manual Setup",
    "title": "2. Manual Setup",
    "content": " ",
    "url": "/manual-setup/",
    
    "relUrl": "/manual-setup/"
  },"22": {
    "doc": "1. Keycloak Setup",
    "title": "Keycloak Authentication Setup",
    "content": "Use this page for the manual per-repository setup. That path assumes you configure Keycloak before deploying the other components. If you are using the Maize MVP path, Keycloak is provisioned/configured automatically; consult this page only if you need to customize it. See Maize Setup for the two setup options. This page will guide you through the steps required to set up Keycloak for the entire service. You will need to configure three Keycloak clients: one for the frontend, one for the backend, and one for the wrapper. Ensure you have administrator access to Keycloak to follow these steps. ",
    "url": "/keycloak/#keycloak-authentication-setup",
    
    "relUrl": "/keycloak/#keycloak-authentication-setup"
  },"23": {
    "doc": "1. Keycloak Setup",
    "title": "Prerequisites",
    "content": "Before starting, ensure you have: . | Access to the Keycloak Administrator UI. | Administrator privileges to create and configure realms, clients, and roles. | . ",
    "url": "/keycloak/#prerequisites",
    
    "relUrl": "/keycloak/#prerequisites"
  },"24": {
    "doc": "1. Keycloak Setup",
    "title": "Steps",
    "content": "1. Log in as an Administrator . | Access your Keycloak instance through the administrator URL. | Log in using your administrator credentials. | . 2. Create a New Realm . | In the Keycloak UI, select Add Realm. | Provide a name for your realm (e.g., my-realm). | Save the new realm. | . 3. Configure the Frontend Client . | Create a Client: . | Navigate to Clients &gt; Create. | Name the client (e.g., frontend-client). | In the Root URL, Admin URL, and Base URL, use your own machine’s IP (placeholder shown below): \"rootUrl\": \"http://YOUR_IP\", \"adminUrl\": \"http://YOUR_IP\", \"baseUrl\": \"http://YOUR_IP\", . | Update the Redirect URIs: \"redirectUris\": [ \"http://YOUR_IP:5173/MainPage\", \"http://YOUR_IP:5173/login\", \"http://YOUR_IP:5173/*\", \"http://YOUR_IP:5114/*\", \"http://YOUR_IP:5173/MainPage/Lab\" ] . | Update the Web Origins: \"webOrigins\": [ \"/*\", \"http://YOUR_IP:5173\", \"http://YOUR_IP:5173/*\", \"http://YOUR_IP:5114/*\" ] . | For Post Logout Redirect URIs, use: \"post.logout.redirect.uris\": \"http://YOUR_IP:5173/*##http://YOUR_IP:5173/login##http://YOUR_IP:5114/*\" . | . | Configure Client Settings: . | Disable Client Authentication and Authorization. | Enable Standard Flow and Direct Access Grants. | . | Assign Roles: . | Go to the Roles tab and add admin and user roles. | . | Create a Frontend-Dedicated Client Scope: . | Navigate to Client Scopes and ensure there is a frontend-dedicated scope. | . | . 4. Configure the Backend Client . | Create a Client: . | Name the client (e.g., backend-client). | In the Authenticator Type, choose Client-Secret. | Save the generated secret for later use. | . | Configure URLs: . | Set the following URLs for the backend: \"rootUrl\": \"http://YOUR_IP:9090\", \"adminUrl\": \"\", \"baseUrl\": \"http://YOUR_IP:9090\", \"redirectUris\": [ \"http://YOUR_IP:9090/*\" ], \"webOrigins\": [ \"\" ] . | . | Configure Client Settings: . | Enable Client Authentication and Authorization. | Enable Standard Flow, Implicit Flow, and Direct Access Grants. | . | Assign Roles: . | Go to the Roles tab and add admin and user roles. | . | Create a Backend-Dedicated Client Scope: . | Navigate to Client Scopes and ensure there is a backend-dedicated scope. | . | . 5. Configure the Wrapper Client . | Create a Client: . | Name the client (e.g., wrapper-client). | No need to configure any URLs for this client. | . | Configure Client Settings: . | Enable Client Authentication. | Enable Standard Flow, Direct Access Grants, and Service Accounts Roles. | . | Assign Roles: . | Add admin roles to the client. | . | Create a Wrapper-Dedicated Client Scope: . | Ensure there is a wrapper-dedicated client scope. | . | . 6. Configure Realm Roles . | Verify Default Roles: . | Go to the Roles section of the realm. | Ensure that there is a role named default-roles. | . | . 7. Create the Admins Group . | Create a Group: . | Navigate to the Groups section. | Create a group named admins. | . | Assign Roles to the Admins Group: . | In the Role Mappings tab of the group, assign roles related to the frontend and backend services. | . | . 8. Create Users and Assign Roles . | Create a User: . | Navigate to the Users section. | Create a new user, providing the required information. | . | Set Password: . | After creating the user, navigate to the Credentials tab. | Set a password for the user. | . | Assign Roles to the User: . | In the Role Mappings tab of the user, assign the default-roles role. | . | Add User to Admins Group: . | Go to the Groups tab and make the user join the admins group. | . | . 9. Adjust Realm Settings . | Session and Token Settings: . | Navigate to the Realm Settings section. | Adjust session duration settings and token settings according to the service’s security requirements. | . | . Final Steps . | After configuring all clients, ensure that each client is properly assigned its dedicated scope and roles. | You can verify the configuration by testing the login and authentication flows for the frontend, backend, and wrapper components. | . ",
    "url": "/keycloak/#steps",
    
    "relUrl": "/keycloak/#steps"
  },"25": {
    "doc": "1. Keycloak Setup",
    "title": "1. Keycloak Setup",
    "content": " ",
    "url": "/keycloak/",
    
    "relUrl": "/keycloak/"
  },"26": {
    "doc": "4. Model Repository Setup",
    "title": "DataCROP Maize Model Repository Deployment",
    "content": "Use this page when following the manual per-repository setup. If you use Maize MVP, the model repository is deployed by the MVP script; refer here only for customization or troubleshooting. See Maize Setup for the two setup options. This is a demo deployment instance for the Maize DataCROP version. It deploys the DataCROP Model Repository infrastructure, consisting of two containers. Requirements . | Docker-CE | . Prerequisites . Before proceeding, make sure you have completed the following steps: . | Airflow Setup: . | Ensure that you have followed the setup instructions for both the Airflow Processing Engine and the Processing Engine Worker. These components need to be properly configured and running before deploying the Maize DataCROP Model Repository. | . | . After completing the setup, follow these steps to configure your environment variables: . | Navigate to your environment variable file (e.g., .env or the relevant configuration file for your deployment). | Update the file with the correct values for your infrastructure. Below are the current values from maize-model-repository/.env and docker-compose.yml; sensitive secrets are redacted—keep using the real values already present in your .env. # Application SERVER_PORT=9090 MAX_FILE_SIZE=200MB MAX_REQUEST_SIZE=500MB # Workflow Management Engine VM_WME_IP=&lt;YOUR_IP&gt; VM_WORKER_IP=&lt;YOUR_IP&gt; WEBSERVER_DAGS_FOLDER=/path/to/maze-processing-engine-airflow/dags WORKER_API_PORT=8090 # Harbor HARBOR_URL=harbor.example.com/ HARBOR_USERNAME=&lt;HARBOR_USERNAME&gt; HARBOR_TOKEN=[REDACTED – keep existing value in your .env] # MongoDB MONGO_INITDB_ROOT_USERNAME=root MONGO_INITDB_ROOT_PASSWORD=[REDACTED – keep existing value in your .env] MONGO_USERNAME=root MONGO_PASSWORD=[REDACTED – keep existing value in your .env] MONGO_DATABASE=registry MONGO_PORT=27017 MONGO_HOST=&lt;MONGO_HOST&gt; # Kafka KAFKA_ENABLED=false KAFKA_BOOTSTRAP_SERVERS=&lt;KAFKA_BOOTSTRAP_SERVERS&gt; # Logstash LOGSTASH_CONFIG_FOLDER=/app/logstash/config/ LOGSTASH_PIPELINE_FOLDER=/app/logstash/pipeline/ # Keycloak KEYCLOAK_ISSUER_URI=https://keycloak.example.com/realms/YOUR-REALM KEYCLOAK_PROVIDER=&lt;KEYCLOAK_PROVIDER&gt; KEYCLOAK_CLIENT_NAME=&lt;KEYCLOAK_CLIENT_NAME&gt; KEYCLOAK_CLIENT_ID=&lt;KEYCLOAK_CLIENT_ID&gt; KEYCLOAK_CLIENT_SECRET=[REDACTED – keep existing value in your .env] KEYCLOAK_SCOPE=openid,offline_access,profile,roles KEYCLOAK_USER_NAME_ATTR=preferred_username KEYCLOAK_JWK_SET_URI=https://keycloak.example.com/realms/YOUR-REALM/protocol/openid-connect/certs # Elastic Stack ELASTIC_VERSION=8.15.3 ELASTIC_PASSWORD=[REDACTED – keep existing value in your .env] LOGSTASH_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] KIBANA_SYSTEM_PASSWORD=[REDACTED – keep existing value in your .env] METRICBEAT_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] FILEBEAT_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] HEARTBEAT_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] MONITORING_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] BEATS_SYSTEM_PASSWORD=[REDACTED – keep existing value in your .env] # Airflow (WME integration) AIRFLOW_BASE_URL=http://&lt;AIRFLOW_HOST&gt;:8080/api/v1 AIRFLOW_USERNAME=&lt;AIRFLOW_USERNAME&gt; AIRFLOW_PASSWORD=[REDACTED – keep existing value in your .env] . Sensitive secrets are redacted above; ensure your .env retains the real values currently configured. | . Optional: Predefining custom processors (before deploying the editor) . | You can ship extra processor definitions for your project by adding a JSON file at config/extra-processors.json in the model repository project. The docker compose mounts it like: . | ./config/extra-processors.json:/app/config/extra-processors.json:ro | . | The file contains an array of processor definitions. Example: . [ { \"name\": \"custom-logstash\", \"type\": \"logstash\", \"description\": \"Moves data from source A to sink B\", \"image\": \"harbor.example.com/project/logstash-custom:1.0.0\", \"parameters\": { \"input\": \"digitalResourceIdA\", \"output\": \"digitalResourceIdB\" } } ] . | Workflow: place your JSON file, deploy the Model Repository, deploy the Workflow Editor, then log in and click Initialize resources (see Workflow Editor Setup). The initialization step loads this file and creates both the default and any extra processors you defined. | . Once these parameters are correctly set, you can proceed with the deployment . Starting the Application . | Navigate to the source directory containing the Dockerfile and docker-compose.yml files. | Run the following commands: . docker build -t wme . docker compose up -d . | . Verifying the Deployment . Wait for the services to start, then run the following commands: . | Check if the WME container is running: . docker ps --filter name=wme-container --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES wme wme-container . | Check if the MongoDB container is running: . docker ps --filter name=mongo --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES mongo:latest mongodb-container . | . Stopping the Application . To stop the containers, run the following command: . docker-compose down . Clean everything up. Run the following command (at your own risk). docker-compose down --volumes --remove-orphans . ",
    "url": "/model-repo/#datacrop-maize-model-repository-deployment",
    
    "relUrl": "/model-repo/#datacrop-maize-model-repository-deployment"
  },"27": {
    "doc": "4. Model Repository Setup",
    "title": "4. Model Repository Setup",
    "content": " ",
    "url": "/model-repo/",
    
    "relUrl": "/model-repo/"
  },"28": {
    "doc": "3. Worker Setup",
    "title": "DataCROP Maize Processing Engine Worker Deployment",
    "content": "Use this page when following the manual per-repository setup. If you use Maize MVP, the worker is deployed by the MVP script; refer here only for customization or troubleshooting. See Maize Setup for the two setup options. This is a demo deployment instance for the Maize DataCROP version. It deploys a Worker responsible for handling tasks within the DataCROP Workflow Management Engine. The deployment consists of a single container. ",
    "url": "/worker/#datacrop-maize-processing-engine-worker-deployment",
    
    "relUrl": "/worker/#datacrop-maize-processing-engine-worker-deployment"
  },"29": {
    "doc": "3. Worker Setup",
    "title": "Overview",
    "content": "The deployment utilizes Apache Airflow and CeleryExecutor for distributed task execution within the DataCROP system. Below is an explanation of the different components and configurations defined in the docker-compose.yml file. ",
    "url": "/worker/#overview",
    
    "relUrl": "/worker/#overview"
  },"30": {
    "doc": "3. Worker Setup",
    "title": "Airflow Worker Setup",
    "content": ". | The airflow-worker service is set up using Airflow’s CeleryExecutor to manage distributed task execution. | The worker communicates with: . | Redis: Used as the message broker for Celery. | PostgreSQL: Used as the backend for storing task results. | . | . ",
    "url": "/worker/#airflow-worker-setup",
    
    "relUrl": "/worker/#airflow-worker-setup"
  },"31": {
    "doc": "3. Worker Setup",
    "title": "Volumes",
    "content": "The following directories are mounted into the Airflow worker container to persist data and provide necessary resources: . | DAGs: Task definitions are stored in the ./dags folder. | Logs: Logs generated by Airflow are stored in the ./logs folder. | Data: Input and output data for tasks are stored in the ./data folder. | Models: Model data is stored in the ./models folder. | Plugins: Airflow plugins can be added via the ./plugins folder. | .env: The .env file is used to handle dynamic environment variables. | . REQUIREMENTS . | Docker-CE | . PREREQUISITES . Before proceeding, ensure that you have followed the setup instructions for the airflow processing engine. After completing the setup, follow these steps to configure your environment variables: . | Navigate to the .env file and ensure that all necessary environment variables are set correctly for your deployment. Current values from maze-processing-engine-worker/.env are shown below; sensitive secrets are redacted—keep using the real values already present in your .env. # HOST || DC.C AIRFLOW_IP=&lt;AIRFLOW_HOST_IP&gt; AIRFLOW_WEB_SECRET_KEY=[REDACTED – keep existing value in your .env] AIRFLOW_FERNET_KEY=[REDACTED – keep existing value in your .env] HOST_IP=&lt;WORKER_HOST_IP&gt; _PIP_ADDITIONAL_REQUIREMENTS='' AIRFLOW_UID=1002 AIRFLOW_GID=0 # WORKER || DC.W WORKER_NAME=&lt;WORKER_NAME&gt; WORKER_SSL_KEY_FILE=/security/${WORKER_NAME}/${WORKER_NAME}-key.pem WORKER_SSL_CERT_FILE=/security/${WORKER_NAME}/${WORKER_NAME}.pem WORKER_SSL_CERT_STORE=/security/ca/rootCA.pem # Please check the GID of the docker group on the host DOCKER_GID=988 # REDIS || DC.C REDIS_TLS_PORT=6379 REDIS_TLS_CERT_FILE=/security/redis/redis.pem REDIS_TLS_KEY_FILE=/security/redis/redis-key.pem REDIS_TLS_CA_CERT_FILE=/security/ca/rootCA.pem REDIS_TLS_CLIENT_CERT_FILE=/security/redis/redis-client.pem REDIS_TLS_CLIENT_KEY_FILE=/security/redis/redis-client-key.pem # CELERY || DC.C CELERY_WEB_UNAME=[REDACTED – keep existing value in your .env] CELERY_WEB_PSSWD=[REDACTED – keep existing value in your .env] . Adjust only if your deployment differs (e.g., different IPs or worker name); do not publish or rotate the redacted secrets already set in your .env. | . Once these parameters are correctly set, you can proceed with the deployment. Start The Application. | Navigate to the source directory containing the docker-compose.yml file. | Run the following command: . docker compose up -d . | . Verify that everything is up and running . Wait for the services to start, then run the following commands: . | Check if the container is running (change worker_name with the actual name that you specified in the .env file): . docker ps --filter name=[worker_name] --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES [worker_name]-airflow-worker [worker_name] . | . Make Sure Everything Works . | Open a browser and navigate to the flower web app (http://{Your IP}:5555/workers). | Enter the credentials provided by your organization for celery. | After successful authentication, you will be redirected to the workers page, where the newly created worker should appear in the workers table. If its status is marked as online, the setup was completed successfully. | . Stop everything. Navigate to the source directory and run the following command. docker-compose down . ",
    "url": "/worker/#volumes",
    
    "relUrl": "/worker/#volumes"
  },"32": {
    "doc": "3. Worker Setup",
    "title": "3. Worker Setup",
    "content": " ",
    "url": "/worker/",
    
    "relUrl": "/worker/"
  },"33": {
    "doc": "Developer Guide",
    "title": "DataCROP Maize Workflow Management Engine Developer Guide",
    "content": " ",
    "url": "/dev-guide/#datacrop-maize-workflow-management-engine-developer-guide",
    
    "relUrl": "/dev-guide/#datacrop-maize-workflow-management-engine-developer-guide"
  },"34": {
    "doc": "Developer Guide",
    "title": "Overview",
    "content": "The Workflow Management Engine application is designed to facilitate efficient creation and management of workflows through four primary tabs in the sidebar: Warehouse, Lab, Airflow, and Kibana. Each tab serves a distinct purpose within the application. | Warehouse: A repository for creating and managing the data models that are used in workflows. | Lab: A dedicated space for designing and configuring workflows, leveraging the data models from the Warehouse. | Airflow: For monitoring the deployment of components. | Kibana: For data visualization. | . ",
    "url": "/dev-guide/#overview",
    
    "relUrl": "/dev-guide/#overview"
  },"35": {
    "doc": "Developer Guide",
    "title": "Developer Guide",
    "content": " ",
    "url": "/dev-guide/",
    
    "relUrl": "/dev-guide/"
  },"36": {
    "doc": "Home",
    "title": "DataCROP™ Information",
    "content": "DataCROP™ (Data Collection Routing &amp; Processing) is a Data collection framework which provides the specifications and relevant implementation to enable a real time data collection, transformation, filtering, and management service to facilitate data consumers (i.e., analytic algorithms). The framework can be applied in IoT environments supporting solutions in various domains (e.g., Industrial, Cybersecurity, etch.). For example, the solution may be used to collect security related data (e.g., network, system, solution proprietary, etch.) from monitored IoT systems and store them to detect patterns of abnormal behaviour by applying simple (i.e., filtering and pre-processing) or more elaborated mechanisms (i.e., AI algorithms). The design of the framework is driven by configurability, extensibility, dynamic setup, stream handling capabilities and Blockchain (Ledger) support. One of the key features of the framework is that it is detached from the underlying infrastructure by employing a specialized data model for modelling the solution’s Data Sources, Processors and Results which facilitates the data interoperability discoverability and configurability of the offered solution. ",
    "url": "/home/#datacrop-information",
    
    "relUrl": "/home/#datacrop-information"
  },"37": {
    "doc": "Home",
    "title": "Demonstrator",
    "content": "A DataCROP™ Farro version demo infrastracture instance, which can be deployed locally, can be found at the farro-demo-deployment-scripts repository. ",
    "url": "/home/#demonstrator",
    
    "relUrl": "/home/#demonstrator"
  },"38": {
    "doc": "Home",
    "title": "Technologies/Framework",
    "content": "DataCROP has been developed and applied in various iterations, within the context of various EU projects, with different dependencies . DataCROP™ Barley (v1.0) Outcome of FAR-EDGE EU Project . | MongoDB | Apache Kafka (Inter-Data Bus) | RabitMQ (Data Input interface) | Kafka Streams (Stream Processing) | NodeJS (Component Implementation) | React (UI) | Hyperledger Fabric (optional Blockchain support) | . DataCROP™ Farro (v2.0) Outcome of Barley version and PROPHESY EU project . | MongoDB | Apache Kafka (Inter-Data Bus communication &amp; input/output interface) | RabbitMQ (Data Input interface) | Algorithms: ** Java Algorithms (Qarma) ** Python Algorithms (RUL feature extraction &amp; Health Index Calculation) ** R Algorithms | NodeJS (Component Implementation) | React (UI) | . DataCROP™ Maize :corn: (V3.0) - (under construction :construction:) Outcome of Farro version and SecureIoT, IoTAC EU projects . | MongoDB | Apache Kafka (Inter-Data Bus) | ELK Stack | Additional to be included as developments evolves | . ",
    "url": "/home/#technologiesframework",
    
    "relUrl": "/home/#technologiesframework"
  },"39": {
    "doc": "Home",
    "title": "Maturity Level / Active years",
    "content": "V1.0 - TRL 6 Designed/Developed and demonstrated under the FAR-EDGE (2016-2019) project . V2.0 - TRL6 Designed/Developed and demonstrated under the H2020 PROPHESY (2017-2020) project. Demonstrated under the QU4LITY (2019-2022) project . V3.0 – TRL4 (Under design/Development) Designed/Developed under the SecureIoT (2018-2021) project, IoTAC (2020-2023), STAR (2020-2023) . ",
    "url": "/home/#maturity-level--active-years",
    
    "relUrl": "/home/#maturity-level--active-years"
  },"40": {
    "doc": "Home",
    "title": "Future/ Interest Steps",
    "content": ". | Intergrade the design/data models/components into one platform/infrastructure independent solution. | Support additional data collection, data processing and data offering services. | Offer an intuitive and user-friendly configuration toolbox (UI). | Offer data visualization mechanisms (UI). | Blockchain (Hyperledger Fabric) integration supporting the configurations and results of the solution. | . ",
    "url": "/home/#future-interest-steps",
    
    "relUrl": "/home/#future-interest-steps"
  },"41": {
    "doc": "Home",
    "title": "Links:",
    "content": ". | Contact DataCROP | Report Issues/Problems | DataCROP Forum | Website (under construction) | DataCROP@DockerHub | Stats@OpenHUB | . ",
    "url": "/home/#links",
    
    "relUrl": "/home/#links"
  },"42": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/home/",
    
    "relUrl": "/home/"
  },"43": {
    "doc": "Index",
    "title": "DataCROP™ Information",
    "content": "DataCROP™ (Data Collection Routing &amp; Processing) is a Data collection framework which provides the specifications and relevant implementation to enable a real time data collection, transformation, filtering, and management service to facilitate data consumers (i.e., analytic algorithms). The framework can be applied in IoT environments supporting solutions in various domains (e.g., Industrial, Cybersecurity, etch.). For example, the solution may be used to collect security related data (e.g., network, system, solution proprietary, etch.) from monitored IoT systems and store them to detect patterns of abnormal behaviour by applying simple (i.e., filtering and pre-processing) or more elaborated mechanisms (i.e., AI algorithms). The design of the framework is driven by configurability, extensibility, dynamic setup, stream handling capabilities and Blockchain (Ledger) support. One of the key features of the framework is that it is detached from the underlying infrastructure by employing a specialized data model for modelling the solution’s Data Sources, Processors and Results which facilitates the data interoperability discoverability and configurability of the offered solution. ",
    "url": "/#datacrop-information",
    
    "relUrl": "/#datacrop-information"
  },"44": {
    "doc": "Index",
    "title": "Demonstrator",
    "content": "A DataCROP™ Farro version demo infrastracture instance, which can be deployed locally, can be found at the farro-demo-deployment-scripts repository. ",
    "url": "/#demonstrator",
    
    "relUrl": "/#demonstrator"
  },"45": {
    "doc": "Index",
    "title": "Technologies/Framework",
    "content": "DataCROP has been developed and applied in various iterations, within the context of various EU projects, with different dependencies . DataCROP™ Barley (v1.0) Outcome of FAR-EDGE EU Project . | MongoDB | Apache Kafka (Inter-Data Bus) | RabitMQ (Data Input interface) | Kafka Streams (Stream Processing) | NodeJS (Component Implementation) | React (UI) | Hyperledger Fabric (optional Blockchain support) | . DataCROP™ Farro (v2.0) Outcome of Barley version and PROPHESY EU project . | MongoDB | Apache Kafka (Inter-Data Bus communication &amp; input/output interface) | RabbitMQ (Data Input interface) | Algorithms: ** Java Algorithms (Qarma) ** Python Algorithms (RUL feature extraction &amp; Health Index Calculation) ** R Algorithms | NodeJS (Component Implementation) | React (UI) | . DataCROP™ Maize :corn: (V3.0) - (under construction :construction:) Outcome of Farro version and SecureIoT, IoTAC EU projects . | MongoDB | Apache Kafka (Inter-Data Bus) | ELK Stack | Additional to be included as developments evolves | . ",
    "url": "/#technologiesframework",
    
    "relUrl": "/#technologiesframework"
  },"46": {
    "doc": "Index",
    "title": "Maturity Level / Active years",
    "content": "V1.0 - TRL 6 Designed/Developed and demonstrated under the FAR-EDGE (2016-2019) project . V2.0 - TRL6 Designed/Developed and demonstrated under the H2020 PROPHESY (2017-2020) project. Demonstrated under the QU4LITY (2019-2022) project . V3.0 – TRL4 (Under design/Development) Designed/Developed under the SecureIoT (2018-2021) project, IoTAC (2020-2023), STAR (2020-2023) . ",
    "url": "/#maturity-level--active-years",
    
    "relUrl": "/#maturity-level--active-years"
  },"47": {
    "doc": "Index",
    "title": "Future/ Interest Steps",
    "content": ". | Intergrade the design/data models/components into one platform/infrastructure independent solution. | Support additional data collection, data processing and data offering services. | Offer an intuitive and user-friendly configuration toolbox (UI). | Offer data visualization mechanisms (UI). | Blockchain (Hyperledger Fabric) integration supporting the configurations and results of the solution. | . ",
    "url": "/#future-interest-steps",
    
    "relUrl": "/#future-interest-steps"
  },"48": {
    "doc": "Index",
    "title": "Links:",
    "content": ". | Contact DataCROP | Report Issues/Problems | DataCROP Forum | Website (under construction :construction:) | DataCROP@DockerHub | Stats@OpenHUB | . ",
    "url": "/#links",
    
    "relUrl": "/#links"
  },"49": {
    "doc": "Index",
    "title": "Index",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"50": {
    "doc": "4. Airflow",
    "title": "Airflow",
    "content": ". For all the workflows we create, two DAGs (Directed Acyclic Graphs) are generated in Airflow: . | Deployment DAG: Responsible for deploying all the processors in the workflow. | Undeployment DAG: Responsible for undeploying all the processors in the workflow. | . This is why there is an Airflow icon in the sidebar, which contains an iframe that takes you directly to the Airflow web page. Additionally, each workflow in the Lab page has an Airflow tab to inspect the corresponding Airflow DAG. ",
    "url": "/airflow-note/#airflow",
    
    "relUrl": "/airflow-note/#airflow"
  },"51": {
    "doc": "4. Airflow",
    "title": "4. Airflow",
    "content": " ",
    "url": "/airflow-note/",
    
    "relUrl": "/airflow-note/"
  },"52": {
    "doc": "2. Creating Data Models",
    "title": "Creating Data Models",
    "content": " ",
    "url": "/creating-data-models/#creating-data-models",
    
    "relUrl": "/creating-data-models/#creating-data-models"
  },"53": {
    "doc": "2. Creating Data Models",
    "title": "Warehouse Tab",
    "content": "My Workflows . Description: This section displays a list of workflows you have created. Features: . | View Workflows: Browse your existing workflows. | Create Workflow: Redirects you to the Lab tab for creating a new workflow. | . My Digital Resources . Description: The Digital Resource entity represents the instantiation of a data source within the system. Purpose: Provides a context and connection interface for accessing and utilizing data. Key Attributes: . | assetID: Identifies the asset. | dataKindID: Links to the type of data the resource manages. | . Functionality: Links physical or digital assets to their operational data within DataCROP. My Workers . Description: This section allows you to view and create worker assets. Purpose: Worker assets are responsible for executing processors within workflows. Features: . | Define worker specifications. | Assign workers to processors for deployment. | . My Data Kinds . Description: The Data Kind entity defines the type of data managed in the system. Purpose: Provides metadata about data structure, format, and type. Key Attributes: . | modelType: Specifies the data’s structural type (e.g., simple or complex). | format: Indicates the format (e.g., JSON, XML). | quantityKind: Defines whether the data is quantitative or qualitative. | . Functionality: Facilitates precise processing and analysis of data. ",
    "url": "/creating-data-models/#warehouse-tab",
    
    "relUrl": "/creating-data-models/#warehouse-tab"
  },"54": {
    "doc": "2. Creating Data Models",
    "title": "2. Creating Data Models",
    "content": " ",
    "url": "/creating-data-models/",
    
    "relUrl": "/creating-data-models/"
  },"55": {
    "doc": "3. Creating Workflows",
    "title": "Creating Workflows",
    "content": " ",
    "url": "/creating-workflows/#creating-workflows",
    
    "relUrl": "/creating-workflows/#creating-workflows"
  },"56": {
    "doc": "3. Creating Workflows",
    "title": "Lab Tab",
    "content": "Creating Workflows . Workflow Specifications Page . Overview: The starting point for creating workflows. Workflow Specifications: . | Name | Description | Configuration: Define the DAG (Directed Acyclic Graph) that represents the workflow in Airflow. | . Flow Creator . Overview: An interactive interface for designing workflows using nodes and edges. Features: . | Add Node: Use the “Add Node” button to add processors to your workflow. | View Workflow: Nodes are displayed in a 2D representation and can be connected using edges. | . Processors . Processor Configuration . | Define the Processor Type and it’s parameters. | Choose a Worker for deployment. | Assign Data Input and Data Output using digital resources. | . Note: Before creating a processor in the Flow Creator, ensure that you have already created the data models you plan to use with it. This includes the worker asset and the digital resources that will represent the data input and output you plan to add. Basic Processor Types Supported: . | Streamhandler: Deploys a full Kafka cluster. | Logstash: Creates a pipeline that transfers data from one digital resource to another. | Kibana: Creates a Logstash pipeline that takes input from a digital resource and transfers it to Elasticsearch for visualization through the workflow editor. | Logstash -&gt; Observation: Creates a Logstash pipeline that takes data from the input digital resource, converts it into an observation, and saves it in a predefined collection. | . ",
    "url": "/creating-workflows/#lab-tab",
    
    "relUrl": "/creating-workflows/#lab-tab"
  },"57": {
    "doc": "3. Creating Workflows",
    "title": "3. Creating Workflows",
    "content": " ",
    "url": "/creating-workflows/",
    
    "relUrl": "/creating-workflows/"
  },"58": {
    "doc": "User Guide",
    "title": "DataCROP Maize Workflow Management Engine User Guide",
    "content": " ",
    "url": "/user-guide/#datacrop-maize-workflow-management-engine-user-guide",
    
    "relUrl": "/user-guide/#datacrop-maize-workflow-management-engine-user-guide"
  },"59": {
    "doc": "User Guide",
    "title": "Overview",
    "content": "The Workflow Management Engine application is designed to facilitate efficient creation and management of workflows through four primary tabs in the sidebar: Warehouse, Lab, Airflow, and Kibana. Each tab serves a distinct purpose within the application. | Warehouse: A repository for creating and managing the data models that are used in workflows. | Lab: A dedicated space for designing and configuring workflows, leveraging the data models from the Warehouse. | Airflow: For monitoring the deployment of components. | Kibana: For data visualization. | . ",
    "url": "/user-guide/#overview",
    
    "relUrl": "/user-guide/#overview"
  },"60": {
    "doc": "User Guide",
    "title": "User Guide",
    "content": " ",
    "url": "/user-guide/",
    
    "relUrl": "/user-guide/"
  },"61": {
    "doc": "1. Getting Started",
    "title": "Getting Started",
    "content": "Before using the Workflow Editor, choose a deployment path on the Maize Setup page: either the single-script Maize MVP approach or the manual per-repository setup. ",
    "url": "/overview/#getting-started",
    
    "relUrl": "/overview/#getting-started"
  },"62": {
    "doc": "1. Getting Started",
    "title": "Accessing the Workflow Editor",
    "content": "To access the Workflow Editor, you need to log in using Keycloak. You can authenticate in one of two ways: . | Using credentials you already have. | Using Single Sign-On (SSO) with your GitHub account. | . ",
    "url": "/overview/#accessing-the-workflow-editor",
    
    "relUrl": "/overview/#accessing-the-workflow-editor"
  },"63": {
    "doc": "1. Getting Started",
    "title": "1. Getting Started",
    "content": " ",
    "url": "/overview/",
    
    "relUrl": "/overview/"
  }
}
