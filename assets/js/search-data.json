{"0": {
    "doc": "Maize Setup",
    "title": "DataCROP Maize Workflow Management Engine Deployment",
    "content": "You can set up the Maize stack in two ways: . | Maize MVP (recommended) — a single repository and script that brings up all Maize components together. Start here if you want the fastest path to a working demo. | Manual per-repository setup — deploy each component yourself (Keycloak, Airflow, Worker, Model Repository, Workflow Editor) for fine-grained control. | . Use the MVP path if you want a quick, reproducible deployment. Choose the manual path if you need to customize each service, change infrastructure details, or operate components independently. ",
    "url": "/Setup/#datacrop-maize-workflow-management-engine-deployment",
    
    "relUrl": "/Setup/#datacrop-maize-workflow-management-engine-deployment"
  },"1": {
    "doc": "Maize Setup",
    "title": "Option 1: Maize MVP (single script)",
    "content": "Follow the instructions in the Maize MVP Setup page to configure the env files in each component folder of the MVP repository and run the one-shot setup script. ",
    "url": "/Setup/#option-1-maize-mvp-single-script",
    
    "relUrl": "/Setup/#option-1-maize-mvp-single-script"
  },"2": {
    "doc": "Maize Setup",
    "title": "Option 2: Manual per-repository setup",
    "content": "If you prefer to deploy each component separately, go to the Manual Setup landing page and follow the component pages in order: . | Keycloak Setup | Airflow Setup | Worker Setup | Model Repository Setup | Workflow Editor Setup | . You can still reference the manual pages even when using the MVP path to understand what each service does and what each environment variable controls. ",
    "url": "/Setup/#option-2-manual-per-repository-setup",
    
    "relUrl": "/Setup/#option-2-manual-per-repository-setup"
  },"3": {
    "doc": "Maize Setup",
    "title": "Maize Setup",
    "content": " ",
    "url": "/Setup/",
    
    "relUrl": "/Setup/"
  },"4": {
    "doc": "Maize MVP Setup",
    "title": "Maize MVP (single-script deployment)",
    "content": "Use the Maize MVP repository to bring up all Maize components with one script. This path is the fastest way to get a working demo; it provisions Keycloak automatically as part of the run. ",
    "url": "/maize-mvp/#maize-mvp-single-script-deployment",
    
    "relUrl": "/maize-mvp/#maize-mvp-single-script-deployment"
  },"5": {
    "doc": "Maize MVP Setup",
    "title": "Repository",
    "content": ". | Maize MVP repo: https://github.com/datacrop/maize-mvp (use your fork/URL if different). | . ",
    "url": "/maize-mvp/#repository",
    
    "relUrl": "/maize-mvp/#repository"
  },"6": {
    "doc": "Maize MVP Setup",
    "title": "Prerequisites",
    "content": ". | Docker installed on the host. | Network/ports open for Keycloak, Airflow, Worker, Model Repository, and Workflow Editor. | . ",
    "url": "/maize-mvp/#prerequisites",
    
    "relUrl": "/maize-mvp/#prerequisites"
  },"7": {
    "doc": "Maize MVP Setup",
    "title": "Configure environment variables",
    "content": ". | Clone the MVP repository and cd into it. | For each component subfolder (Keycloak, Airflow, Worker, Model Repository, Workflow Editor, etc.), open its .env or config file and adjust values for your environment (IP/hostnames, ports, credentials where applicable). | Use the corresponding manual pages for variable meanings if needed: . | Keycloak, Airflow, Worker, Model Repository, Workflow Editor | . | . ",
    "url": "/maize-mvp/#configure-environment-variables",
    
    "relUrl": "/maize-mvp/#configure-environment-variables"
  },"8": {
    "doc": "Maize MVP Setup",
    "title": "Run the setup script",
    "content": ". | In the repo root, run the provided setup script (typically ./scripts/setup.sh; mark executable first if needed: chmod +x scripts/setup.sh). | The script will run docker compose up for all components. Wait until containers are healthy. | . ",
    "url": "/maize-mvp/#run-the-setup-script",
    
    "relUrl": "/maize-mvp/#run-the-setup-script"
  },"9": {
    "doc": "Maize MVP Setup",
    "title": "Verify the deployment",
    "content": ". | Check containers: . | docker ps --filter name=keycloak | docker ps --filter name=airflow | docker ps --filter name=worker | docker ps --filter name=wme (model repo) | docker ps --filter name=wme-ui (workflow editor) | . | Open the Workflow Editor UI at http://&lt;WME_UI_HOST&gt;:5173/MainPage/Warehouse and confirm it loads. | . ",
    "url": "/maize-mvp/#verify-the-deployment",
    
    "relUrl": "/maize-mvp/#verify-the-deployment"
  },"10": {
    "doc": "Maize MVP Setup",
    "title": "Post-deployment (required)",
    "content": "After the Workflow Editor UI is reachable: . | Log in via Keycloak. | Go to Settings in the editor UI. | (Optional) Add predefined processor definitions by placing extra-processors.json in the Model Repository component’s config/ folder (copy from extra-processors.example.json) before running the MVP script / before initialization (see Model Repository Setup for schema details). | Click Initialize resources (see Workflow Editor Setup for details). | This creates the base resources and any extra processors defined via the Model Repository. Skipping this leaves the system uninitialized. | . ",
    "url": "/maize-mvp/#post-deployment-required",
    
    "relUrl": "/maize-mvp/#post-deployment-required"
  },"11": {
    "doc": "Maize MVP Setup",
    "title": "Notes on Keycloak",
    "content": ". | In the MVP path, Keycloak is provisioned/configured automatically by the script; you do not need a pre-existing Keycloak instance. | If you need to customize Keycloak manually, refer to the Keycloak Setup page. | . ",
    "url": "/maize-mvp/#notes-on-keycloak",
    
    "relUrl": "/maize-mvp/#notes-on-keycloak"
  },"12": {
    "doc": "Maize MVP Setup",
    "title": "Maize MVP Setup",
    "content": " ",
    "url": "/maize-mvp/",
    
    "relUrl": "/maize-mvp/"
  },"13": {
    "doc": "2. Airflow Setup",
    "title": "DataCROP Maize Airflow Processing Engine Deployment",
    "content": "Use this page when following the manual per-repository setup. If you use Maize MVP, this component is deployed by the MVP script; refer here only for customization or troubleshooting. See Maize Setup for the two setup options. This is a demo deployment instance for the Maize DataCROP version. It deploys the Airflow web server responsible for managing tasks within the DataCROP Workflow Management Engine infrastructure. The deployment consists of six containers. ",
    "url": "/airflow/#datacrop-maize-airflow-processing-engine-deployment",
    
    "relUrl": "/airflow/#datacrop-maize-airflow-processing-engine-deployment"
  },"14": {
    "doc": "2. Airflow Setup",
    "title": "Overview",
    "content": "The DataCROP Maize Airflow Processing Engine is a critical component of the DataCROP Workflow Management Engine. This engine is responsible for orchestrating and managing the execution of various tasks (DAGs) within the DataCROP infrastructure, providing an interface to monitor and manage workflows through the Airflow webserver. ",
    "url": "/airflow/#overview",
    
    "relUrl": "/airflow/#overview"
  },"15": {
    "doc": "2. Airflow Setup",
    "title": "Requirements",
    "content": ". | Docker-CE | . ",
    "url": "/airflow/#requirements",
    
    "relUrl": "/airflow/#requirements"
  },"16": {
    "doc": "2. Airflow Setup",
    "title": "Prerequisites",
    "content": "Before proceeding with the deployment, make sure to complete the following steps: . After completing the setup, follow these steps to configure your environment variables: . | In the Airflow Processing Engine repository, edit its .env file and ensure that all necessary environment variables are set correctly for your deployment. Current values from maze-processing-engine-airflow/.env are shown below; sensitive secrets are redacted—keep using the real values already present in your .env. # AIRFLOW USERS || DC.C AIRFLOW_UID=1002 DOCKER_GID=988 AIRFLOW_WEB_PORT=8080 AIRFLOW_WWW_UNAME_USERNAME='airflow' AIRFLOW_WEB_PSSWD=[REDACTED – keep existing value in your .env] AIRFLOW_WEB_SSL_CERT=/security/airflow/airflow.pem AIRFLOW_WEB_SSL_KEY=/security/airflow/airflow-key.pem AIRFLOW_CA_CERT=/security/ca/rootCA.pem AIRFLOW_WEB_SECRET_KEY=[REDACTED – keep existing value in your .env] AIRFLOW_FERNET_KEY=[REDACTED – keep existing value in your .env] # HOST || DC.C HOST_IP=&lt;HOST_IP&gt; # WORKER_NAME=worker01 # REDIS || DC.C REDIS_TLS_PORT=6379 REDIS_TLS_CERT_FILE=/security/redis/redis.pem REDIS_TLS_KEY_FILE=/security/redis/redis-key.pem REDIS_TLS_CA_CERT_FILE=/security/ca/rootCA.pem REDIS_TLS_CLIENT_CERT_FILE=/security/redis/redis-client.pem REDIS_TLS_CLIENT_KEY_FILE=/security/redis/redis-client-key.pem # POSTGRES || DC.C POSTGRES_PORT=5432 POSTGRES_SSL_CERT_FILE=/security/postgres/postgres.pem POSTGRES_SSL_KEY_FILE=/security/postgres/postgres-key.pem POSTGRES_SSL_CA_FILE=/security/ca/rootCA.pem # Celery || DC.C CELERY_WEB_UNAME=[REDACTED – keep existing value in your .env] CELERY_WEB_PSSWD=[REDACTED – keep existing value in your .env] CELERY_FLOWER_PORT=5555 CELERY_FLOWER_CERT=/security/flower/flower.pem CELERY_FLOWER_KEY=/security/flower/flower-key.pem CELERY_FLOWER_CA_CERT=/security/ca/rootCA.pem REMOTE_WORKER_NAME=&lt;REMOTE_WORKER_NAME&gt; REMOTE_WORKER_IP=&lt;REMOTE_WORKER_IP&gt; . Sensitive secrets are redacted above; ensure your .env retains the real values currently configured. | . Once these parameters are correctly set, you can proceed with the deployment. Start The Application. | Navigate to the source directory containing the docker-compose.yml file. | Run the following command: . docker compose up -d . | . Verify that everything is up and running . Wait for the services to start, then run the following commands: . | Check if the container is running (change worker_name with the actual name that you specified in the .env file): . docker ps --filter name=airflow --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES airflow-airflow-triggerer airflow-airflow-triggerer-1 airflow-airflow-webserver airflow-airflow-webserver-1 airflow-flower airflow-flower-1 airflow-airflow-scheduler airflow-airflow-scheduler-1 postgres:13 airflow-postgres-1 redis:7.2 airflow-redis-1 . | . Make Sure Everything Works . | Access the Flower Web App: . | Open a browser and navigate to http://{Your IP}:5555/workers. | Log in using the celery credentials you provided in the .env file. | After successful authentication, you should be redirected to the workers page, confirming that Celery was set up correctly. | . | Access the Airflow Web App: . | Open a browser and go to http://{Your IP}:8080/home. | Log in using the credentials provided by your organization for Airflow. | You should see all the available DAGs listed, confirming that your DAGs folder is properly configured. | . | . Stop everything. Navigate to the source directory and run the following command. docker-compose down . ",
    "url": "/airflow/#prerequisites",
    
    "relUrl": "/airflow/#prerequisites"
  },"17": {
    "doc": "2. Airflow Setup",
    "title": "2. Airflow Setup",
    "content": " ",
    "url": "/airflow/",
    
    "relUrl": "/airflow/"
  },"18": {
    "doc": "5. Workflow Editor Setup",
    "title": "DataCROP Maize Workflow Management Editor Deployment",
    "content": "This is a demo deployment instance for the Maize DataCROP version. It deploys the DataCROP Workflow Management Engine web application for creating and managing workflows. The deployment consists of a single container. Use this page when following the manual per-repository setup. If you use Maize MVP, the editor is deployed by the MVP script; follow the initialization steps below once it is up. See Maize Setup for the two setup options. Prerequisites . Before proceeding, ensure you have completed the setup instructions for the Maize Model Repository. After completing the setup, follow these steps to configure your environment variables: . | In the Workflow Editor repository, edit ui/.env. | Update the file with the correct values for your infrastructure. Current values from maze-workflow-management-editor/ui/.env are: . VITE_API_URL=http://&lt;WME_API_HOST&gt;:9090 VITE_BACKEND_IP=&lt;WME_API_HOST&gt; VITE_AIRFLOW_IP=&lt;AIRFLOW_HOST&gt; VITE_KEYCLOAK_URL=https://&lt;KEYCLOAK_HOST&gt;/ VITE_KEYCLOAK_REALM=&lt;KEYCLOAK_REALM&gt; VITE_KEYCLOAK_CLIENT_ID=&lt;KEYCLOAK_CLIENT_ID&gt; VITE_PROJECT_NAME=&lt;PROJECT_NAME&gt; VITE_DEFAULT_PRIMARY_COLOR=#DA3333 . Adjust the above values only if your deployment differs (e.g., different host IP or Keycloak realm). No secrets are stored in this file. | . Once these parameters are correctly set, you can proceed with the deployment. REQUIREMENTS . | Docker-CE | . Start The Application. | Clone the repository and navigate to the source directory containing the docker-compose.yml file. | Run the following command: . docker compose up -d . | . Verify that everything is up and running . Wait for the services to start, then run the following commands: . | Check if the container is running: . docker ps --filter name=wme-ui --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES node:lts-alpine wme-ui . | Check if the network is set up correctly: . docker network ls --filter name=\"maze-workflow-management-editor_wme-network\" --format \"table {{.Name}}\\t{{.Driver}}\\t{{.Scope}}\" . Ensure the output matches the following: . NAME DRIVER SCOPE maze-workflow-management-editor_wme-network bridge local . | . Post-deployment initialization (required) . Perform this step after the UI is reachable (applies to both Maize MVP and manual setups): . | Log in through Keycloak. | In the Workflow Editor UI, open Settings. | Click Initialize resources. | . If you want instance-specific predefined processors, configure config/extra-processors.json in the Model Repository before clicking Initialize resources; use config/extra-processors.example.json as a template. What this does: . | Creates the base data models, workers, asset categories, data interface types, and processor definitions needed for the application to function. | Imports any custom processors defined in config/extra-processors.json of the Model Repository (see Model Repository Setup). | . Run this once per environment after deployments. Skipping it leaves the system uninitialized. Make Sure Everything Works . | Open a browser and go to http://&lt;WME_UI_HOST&gt;:5173/MainPage/Warehouse. | You will be redirected to the Keycloak authentication page. Enter the credentials provided by your organization. | After successful authentication, you will be redirected to the main page of the workflow management engine, where you can begin creating and managing workflows. | . Stop everything. Navigate to the source directory and run the following command. docker-compose down . ",
    "url": "/editor/#datacrop-maize-workflow-management-editor-deployment",
    
    "relUrl": "/editor/#datacrop-maize-workflow-management-editor-deployment"
  },"19": {
    "doc": "5. Workflow Editor Setup",
    "title": "5. Workflow Editor Setup",
    "content": " ",
    "url": "/editor/",
    
    "relUrl": "/editor/"
  },"20": {
    "doc": "Manual Setup",
    "title": "Manual per-repository setup",
    "content": "Use this path if you want to deploy and customize each component separately. Follow the component pages in order: . | Keycloak Setup | Airflow Setup | Worker Setup | Model Repository Setup | Workflow Editor Setup | . You can still reference these pages even if you use the Maize MVP path, to understand variables and configuration details. ",
    "url": "/manual-setup/#manual-per-repository-setup",
    
    "relUrl": "/manual-setup/#manual-per-repository-setup"
  },"21": {
    "doc": "Manual Setup",
    "title": "Manual Setup",
    "content": " ",
    "url": "/manual-setup/",
    
    "relUrl": "/manual-setup/"
  },"22": {
    "doc": "1. Keycloak Setup",
    "title": "Keycloak Authentication Setup",
    "content": "Use this page for the manual per-repository setup. That path assumes you configure Keycloak before deploying the other components. If you are using the Maize MVP path, Keycloak is provisioned/configured automatically; consult this page only if you need to customize it. See Maize Setup for the two setup options. This page will guide you through the steps required to set up Keycloak for the entire service. You will need to configure three Keycloak clients: one for the frontend, one for the backend, and one for the wrapper. Ensure you have administrator access to Keycloak to follow these steps. ",
    "url": "/keycloak/#keycloak-authentication-setup",
    
    "relUrl": "/keycloak/#keycloak-authentication-setup"
  },"23": {
    "doc": "1. Keycloak Setup",
    "title": "Token contents expected by the backend",
    "content": "The Model Repository backend relies on information inside the JWT to identify the current user. | The backend expects a custom claim named userId to exist in the access token. If it is missing, API calls that depend on the current user will fail with “User not found”. | The backend also reads roles from the token (realm roles and/or client roles). The deployment commonly uses roles such as admin and super-admin. | . If your realm does not already provide a userId claim, add a Keycloak protocol mapper so the access token contains it (for example mapping to the user UUID or another unique user identifier used by your deployment). ",
    "url": "/keycloak/#token-contents-expected-by-the-backend",
    
    "relUrl": "/keycloak/#token-contents-expected-by-the-backend"
  },"24": {
    "doc": "1. Keycloak Setup",
    "title": "Prerequisites",
    "content": "Before starting, ensure you have: . | Access to the Keycloak Administrator UI. | Administrator privileges to create and configure realms, clients, and roles. | . ",
    "url": "/keycloak/#prerequisites",
    
    "relUrl": "/keycloak/#prerequisites"
  },"25": {
    "doc": "1. Keycloak Setup",
    "title": "Steps",
    "content": "1. Log in as an Administrator . | Access your Keycloak instance through the administrator URL. | Log in using your administrator credentials. | . 2. Create a New Realm . | In the Keycloak UI, select Add Realm. | Provide a name for your realm (e.g., my-realm). | Save the new realm. | . 3. Configure the Frontend Client . | Create a Client: . | Navigate to Clients &gt; Create. | Name the client (e.g., frontend-client). | In the Root URL, Admin URL, and Base URL, use your own machine’s IP (placeholder shown below): \"rootUrl\": \"http://YOUR_IP\", \"adminUrl\": \"http://YOUR_IP\", \"baseUrl\": \"http://YOUR_IP\", . | Update the Redirect URIs: \"redirectUris\": [ \"http://YOUR_IP:5173/MainPage\", \"http://YOUR_IP:5173/login\", \"http://YOUR_IP:5173/*\", \"http://YOUR_IP:5114/*\", \"http://YOUR_IP:5173/MainPage/Lab\" ] . | Update the Web Origins: \"webOrigins\": [ \"/*\", \"http://YOUR_IP:5173\", \"http://YOUR_IP:5173/*\", \"http://YOUR_IP:5114/*\" ] . | For Post Logout Redirect URIs, use: \"post.logout.redirect.uris\": \"http://YOUR_IP:5173/*##http://YOUR_IP:5173/login##http://YOUR_IP:5114/*\" . | . | Configure Client Settings: . | Disable Client Authentication and Authorization. | Enable Standard Flow and Direct Access Grants. | . | Assign Roles: . | Go to the Roles tab and add admin and user roles. | . | Create a Frontend-Dedicated Client Scope: . | Navigate to Client Scopes and ensure there is a frontend-dedicated scope. | . | . 4. Configure the Backend Client . | Create a Client: . | Name the client (e.g., backend-client). | In the Authenticator Type, choose Client-Secret. | Save the generated secret for later use. | . | Configure URLs: . | Set the following URLs for the backend: \"rootUrl\": \"http://YOUR_IP:9090\", \"adminUrl\": \"\", \"baseUrl\": \"http://YOUR_IP:9090\", \"redirectUris\": [ \"http://YOUR_IP:9090/*\" ], \"webOrigins\": [ \"\" ] . | . | Configure Client Settings: . | Enable Client Authentication and Authorization. | Enable Standard Flow, Implicit Flow, and Direct Access Grants. | . | Assign Roles: . | Go to the Roles tab and add admin and user roles. | . | Create a Backend-Dedicated Client Scope: . | Navigate to Client Scopes and ensure there is a backend-dedicated scope. | . | . 5. Configure the Wrapper Client . | Create a Client: . | Name the client (e.g., wrapper-client). | No need to configure any URLs for this client. | . | Configure Client Settings: . | Enable Client Authentication. | Enable Standard Flow, Direct Access Grants, and Service Accounts Roles. | . | Assign Roles: . | Add admin roles to the client. | . | Create a Wrapper-Dedicated Client Scope: . | Ensure there is a wrapper-dedicated client scope. | . | . 6. Configure Realm Roles . | Verify Default Roles: . | Go to the Roles section of the realm. | Ensure that there is a role named default-roles. | . | . 7. Create the Admins Group . | Create a Group: . | Navigate to the Groups section. | Create a group named admins. | . | Assign Roles to the Admins Group: . | In the Role Mappings tab of the group, assign roles related to the frontend and backend services. | . | . 8. Create Users and Assign Roles . | Create a User: . | Navigate to the Users section. | Create a new user, providing the required information. | . | Set Password: . | After creating the user, navigate to the Credentials tab. | Set a password for the user. | . | Assign Roles to the User: . | In the Role Mappings tab of the user, assign the default-roles role. | . | Add User to Admins Group: . | Go to the Groups tab and make the user join the admins group. | . | . 9. Adjust Realm Settings . | Session and Token Settings: . | Navigate to the Realm Settings section. | Adjust session duration settings and token settings according to the service’s security requirements. | . | . Final Steps . | After configuring all clients, ensure that each client is properly assigned its dedicated scope and roles. | You can verify the configuration by testing the login and authentication flows for the frontend, backend, and wrapper components. | . ",
    "url": "/keycloak/#steps",
    
    "relUrl": "/keycloak/#steps"
  },"26": {
    "doc": "1. Keycloak Setup",
    "title": "1. Keycloak Setup",
    "content": " ",
    "url": "/keycloak/",
    
    "relUrl": "/keycloak/"
  },"27": {
    "doc": "4. Model Repository Setup",
    "title": "DataCROP Maize Model Repository Deployment",
    "content": "Use this page when following the manual per-repository setup. If you use Maize MVP, the model repository is deployed by the MVP script; refer here only for customization or troubleshooting. See Maize Setup for the two setup options. This is a demo deployment instance for the Maize DataCROP version. It deploys the DataCROP Model Repository infrastructure, consisting of the WME server plus supporting containers (MongoDB and the Elastic Stack services used by Logstash/Kibana pipelines). Requirements . | Docker-CE | . Prerequisites . Before proceeding, make sure you have completed the following steps: . | Airflow Setup: . | Ensure that you have followed the setup instructions for both the Airflow Processing Engine and the Processing Engine Worker. These components need to be properly configured and running before deploying the Maize DataCROP Model Repository. | . | . After completing the setup, follow these steps to configure your environment variables: . | Navigate to your environment variable file (e.g., .env or the relevant configuration file for your deployment). | Update the file with the correct values for your infrastructure. Below are the current values from maize-model-repository/.env and docker-compose.yml; sensitive secrets are redacted—keep using the real values already present in your .env. # Application SERVER_PORT=9090 MAX_FILE_SIZE=200MB MAX_REQUEST_SIZE=500MB # Workflow Management Engine VM_WME_IP=&lt;YOUR_IP&gt; VM_WORKER_IP=&lt;YOUR_IP&gt; WEBSERVER_DAGS_FOLDER=/path/to/maze-processing-engine-airflow/dags WORKER_API_PORT=8090 # Harbor HARBOR_URL=harbor.example.com/ HARBOR_USERNAME=&lt;HARBOR_USERNAME&gt; HARBOR_TOKEN=[REDACTED – keep existing value in your .env] # MongoDB MONGO_INITDB_ROOT_USERNAME=root MONGO_INITDB_ROOT_PASSWORD=[REDACTED – keep existing value in your .env] MONGO_USERNAME=root MONGO_PASSWORD=[REDACTED – keep existing value in your .env] MONGO_DATABASE=registry MONGO_PORT=27017 MONGO_HOST=&lt;MONGO_HOST&gt; # Kafka KAFKA_ENABLED=false KAFKA_BOOTSTRAP_SERVERS=&lt;KAFKA_BOOTSTRAP_SERVERS&gt; # Logstash LOGSTASH_CONFIG_FOLDER=/app/logstash/config/ LOGSTASH_PIPELINE_FOLDER=/app/logstash/pipeline/ # Keycloak KEYCLOAK_ISSUER_URI=https://keycloak.example.com/realms/YOUR-REALM KEYCLOAK_PROVIDER=&lt;KEYCLOAK_PROVIDER&gt; KEYCLOAK_CLIENT_NAME=&lt;KEYCLOAK_CLIENT_NAME&gt; KEYCLOAK_CLIENT_ID=&lt;KEYCLOAK_CLIENT_ID&gt; KEYCLOAK_CLIENT_SECRET=[REDACTED – keep existing value in your .env] KEYCLOAK_SCOPE=openid,offline_access,profile,roles KEYCLOAK_USER_NAME_ATTR=preferred_username KEYCLOAK_JWK_SET_URI=https://keycloak.example.com/realms/YOUR-REALM/protocol/openid-connect/certs # Elastic Stack ELASTIC_VERSION=8.15.3 ELASTIC_PASSWORD=[REDACTED – keep existing value in your .env] LOGSTASH_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] KIBANA_SYSTEM_PASSWORD=[REDACTED – keep existing value in your .env] METRICBEAT_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] FILEBEAT_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] HEARTBEAT_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] MONITORING_INTERNAL_PASSWORD=[REDACTED – keep existing value in your .env] BEATS_SYSTEM_PASSWORD=[REDACTED – keep existing value in your .env] # Airflow (WME integration) AIRFLOW_BASE_URL=http://&lt;AIRFLOW_HOST&gt;:8080/api/v1 AIRFLOW_USERNAME=&lt;AIRFLOW_USERNAME&gt; AIRFLOW_PASSWORD=[REDACTED – keep existing value in your .env] . Sensitive secrets are redacted above; ensure your .env retains the real values currently configured. | . Defaults created by Initialize resources . When a Workflow Editor user clicks Settings → Initialize resources, the Model Repository seeds a baseline catalog (only if the resources don’t already exist). This includes default data interface types and processor definitions. Default data interface types . These interface types are intentionally aligned with the editor’s automatic Logstash pipeline creation: the editor uses these type names and fields to generate Logstash input/output configuration automatically. | elasticsearch . | hosts: 167.235.128.77:9200 | user: logstash_internal | password: ${LOGSTASH_INTERNAL_PASSWORD} | index: test_index | . | kafka . | bootstrap_servers: 167.235.128.77:9092 | topic_id: giannis_processed | . | http . | url: http://localhost:8080/api | port: 8080 | http_method: post | format: json | user: \"\" | password: \"\" | . | mongodb . | uri: mongodb://localhost:27017/mydb | database: mydb | collection: mycollection | . | s3 . | bucket: my-bucket | region: eu-central-1 | endpoint: \"\" (empty means AWS S3; otherwise can be e.g. http://minio:9000) | access_key_id: \"\" | secret_access_key: \"\" | prefix: logs/ | . | redis . | host: localhost | port: 6379 | data_type: list (supported: list, channel, pattern_channel) | key: mylist | password: \"\" | . | rabbitmq . | host: localhost | port: 5672 | user: guest | password: guest | queue: myqueue (input) | exchange: myexchange (output) | exchange_type: direct (supported: direct, topic, fanout) | vhost: / | . | . Default processor definitions . Initialize resources also creates these processor definitions (if missing): . | Apache Kafka (Data Persistence, 0.1) — provisions Kafka + AKHQ. | Kibana Pipeline (Datacrop Service, 1.0) — enables the built-in Kibana pipeline (active=true). | Logstash Pipeline (Datacrop Service, 1.0) — enables the built-in Logstash pipeline (active=true); logstash_filter defaults to empty and expects filter plugin content only (no filter {} wrapper). | . Optional: Predefining processor definitions (before initialization) . In addition to the defaults above, deployers can ship predefined processor definitions that will be imported when a Workflow Editor user clicks Initialize resources. This lets each deployed instance come up with a customized processor catalog. How to use . | Create config/extra-processors.json (use the template file as a starting point): . | cp config/extra-processors.example.json config/extra-processors.json | . | Edit config/extra-processors.json: . | Kafka is just an example in the template; rename the processor name and/or replace the entry with your own processors. | . | Ensure the file is mounted into the Model Repository container (already present in docker-compose.yml): . | ./config/extra-processors.json:/app/config/extra-processors.json:ro | . | Deploy the Model Repository, then in the Workflow Editor go to Settings → Initialize resources (see Workflow Editor Setup). | . File format (schema) . The server expects a root object with a processors array: . | Root: { \"processors\": [ ... ] } | Each processor: . | name, description, processorType, version, copyright, processorLocation, fontAwesomeIcon, projectName, containerImage | parameters: a list of { \"name\", \"description\", \"type\", \"defaultValue\" } | . | . Example (from config/extra-processors.example.json): . { \"processors\": [ { \"name\": \"Kafka Example\", \"description\": \"This processor is used for building a Kafka cluster alongside the akhq frontend for visualizations\", \"processorType\": \"Data Persistence\", \"version\": \"0.1\", \"copyright\": \"Apache\", \"processorLocation\": \"Local Deployment\", \"fontAwesomeIcon\": \"fa-solid fa-bus\", \"projectName\": \"test\", \"containerImage\": \"\", \"parameters\": [ { \"name\": \"KAFKA_NETWORK\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"kafka-network\" }, { \"name\": \"KAFKA_DATA\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"kafka-data\" }, { \"name\": \"KAFKA_HOSTNAME\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"kafka\" }, { \"name\": \"KAFKA_CONTAINER_NAME\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"kafka\" }, { \"name\": \"KAFKA_EXTERNAL_PORT\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"9092\" }, { \"name\": \"KAFKA_EXTERNAL_HOSTNAME_OR_IP\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"167.235.128.77\" }, { \"name\": \"KAFKA_INTERNAL_PORT\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"9094\" }, { \"name\": \"KAFKA_INTERNAL_HOSTNAME_OR_IP\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"kafka\" }, { \"name\": \"CLUSTER_ID\", \"description\": \"kraft mode cluster id\", \"type\": \"String\", \"defaultValue\": \"cluster-id\" }, { \"name\": \"AKHQ_CONTAINER_NAME\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"akhq\" }, { \"name\": \"AKHQ_IMAGE\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"0.24.0\" }, { \"name\": \"AKHQ_PORT\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"8081\" }, { \"name\": \"AKHQ_CONNECTION_NAME_PREFIX\", \"description\": \"\", \"type\": \"String\", \"defaultValue\": \"kafka-connection\" } ] } ] } . Behavior notes . | If the file is missing or invalid, initialization continues without failing. | If a processor definition with the same name already exists, it is skipped (not overwritten). | Changing name (for example, adding v2) creates a separate processor definition. | . Notes about default values created during initialization . The Initialize resources action creates the default interface templates and processor definitions listed above with deployment-specific defaults (for example IPs/ports for Elasticsearch/Kafka). Review and update the created entities in the UI after initialization if the defaults do not match your environment. Once these parameters are correctly set, you can proceed with the deployment . Starting the Application . | Navigate to the source directory containing the Dockerfile and docker-compose.yml files. | Run the following commands: . docker build -t wme . docker compose up -d . | . Verifying the Deployment . Wait for the services to start, then run the following commands: . | Check if the WME container is running: . docker ps --filter name=wme-container --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES wme wme-container . | Check if the MongoDB container is running: . docker ps --filter name=mongo --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES mongo:latest mongodb-container . | . Stopping the Application . To stop the containers, run the following command: . docker-compose down . Clean everything up. Run the following command (at your own risk). docker-compose down --volumes --remove-orphans . ",
    "url": "/model-repo/#datacrop-maize-model-repository-deployment",
    
    "relUrl": "/model-repo/#datacrop-maize-model-repository-deployment"
  },"28": {
    "doc": "4. Model Repository Setup",
    "title": "4. Model Repository Setup",
    "content": " ",
    "url": "/model-repo/",
    
    "relUrl": "/model-repo/"
  },"29": {
    "doc": "3. Worker Setup",
    "title": "DataCROP Maize Processing Engine Worker Deployment",
    "content": "Use this page when following the manual per-repository setup. If you use Maize MVP, the worker is deployed by the MVP script; refer here only for customization or troubleshooting. See Maize Setup for the two setup options. This is a demo deployment instance for the Maize DataCROP version. It deploys a Worker responsible for handling tasks within the DataCROP Workflow Management Engine. The deployment consists of a single container. ",
    "url": "/worker/#datacrop-maize-processing-engine-worker-deployment",
    
    "relUrl": "/worker/#datacrop-maize-processing-engine-worker-deployment"
  },"30": {
    "doc": "3. Worker Setup",
    "title": "Overview",
    "content": "The deployment utilizes Apache Airflow and CeleryExecutor for distributed task execution within the DataCROP system. Below is an explanation of the different components and configurations defined in the docker-compose.yml file. ",
    "url": "/worker/#overview",
    
    "relUrl": "/worker/#overview"
  },"31": {
    "doc": "3. Worker Setup",
    "title": "Airflow Worker Setup",
    "content": ". | The airflow-worker service is set up using Airflow’s CeleryExecutor to manage distributed task execution. | The worker communicates with: . | Redis: Used as the message broker for Celery. | PostgreSQL: Used as the backend for storing task results. | . | . ",
    "url": "/worker/#airflow-worker-setup",
    
    "relUrl": "/worker/#airflow-worker-setup"
  },"32": {
    "doc": "3. Worker Setup",
    "title": "Volumes",
    "content": "The following directories are mounted into the Airflow worker container to persist data and provide necessary resources: . | DAGs: Task definitions are stored in the ./dags folder. | Logs: Logs generated by Airflow are stored in the ./logs folder. | Data: Input and output data for tasks are stored in the ./data folder. | Models: Model data is stored in the ./models folder. | Plugins: Airflow plugins can be added via the ./plugins folder. | .env: The .env file is used to handle dynamic environment variables. | . REQUIREMENTS . | Docker-CE | . PREREQUISITES . Before proceeding, ensure that you have followed the setup instructions for the airflow processing engine. After completing the setup, follow these steps to configure your environment variables: . | In the Processing Engine Worker repository, edit its .env file and ensure that all necessary environment variables are set correctly for your deployment. Current values from maze-processing-engine-worker/.env are shown below; sensitive secrets are redacted—keep using the real values already present in your .env. # HOST || DC.C AIRFLOW_IP=&lt;AIRFLOW_HOST_IP&gt; AIRFLOW_WEB_SECRET_KEY=[REDACTED – keep existing value in your .env] AIRFLOW_FERNET_KEY=[REDACTED – keep existing value in your .env] HOST_IP=&lt;WORKER_HOST_IP&gt; _PIP_ADDITIONAL_REQUIREMENTS='' AIRFLOW_UID=1002 AIRFLOW_GID=0 # WORKER || DC.W WORKER_NAME=&lt;WORKER_NAME&gt; WORKER_SSL_KEY_FILE=/security/${WORKER_NAME}/${WORKER_NAME}-key.pem WORKER_SSL_CERT_FILE=/security/${WORKER_NAME}/${WORKER_NAME}.pem WORKER_SSL_CERT_STORE=/security/ca/rootCA.pem # Please check the GID of the docker group on the host DOCKER_GID=988 # REDIS || DC.C REDIS_TLS_PORT=6379 REDIS_TLS_CERT_FILE=/security/redis/redis.pem REDIS_TLS_KEY_FILE=/security/redis/redis-key.pem REDIS_TLS_CA_CERT_FILE=/security/ca/rootCA.pem REDIS_TLS_CLIENT_CERT_FILE=/security/redis/redis-client.pem REDIS_TLS_CLIENT_KEY_FILE=/security/redis/redis-client-key.pem # CELERY || DC.C CELERY_WEB_UNAME=[REDACTED – keep existing value in your .env] CELERY_WEB_PSSWD=[REDACTED – keep existing value in your .env] . Adjust only if your deployment differs (e.g., different IPs or worker name); do not publish or rotate the redacted secrets already set in your .env. | . Once these parameters are correctly set, you can proceed with the deployment. Start The Application. | Navigate to the source directory containing the docker-compose.yml file. | Run the following command: . docker compose up -d . | . Verify that everything is up and running . Wait for the services to start, then run the following commands: . | Check if the container is running (change worker_name with the actual name that you specified in the .env file): . docker ps --filter name=[worker_name] --format \"table {{.Image}}\\t{{.Names}}\" . You should see the following output: . IMAGE NAMES [worker_name]-airflow-worker [worker_name] . | . Make Sure Everything Works . | Open a browser and navigate to the flower web app (http://{Your IP}:5555/workers). | Enter the credentials provided by your organization for celery. | After successful authentication, you will be redirected to the workers page, where the newly created worker should appear in the workers table. If its status is marked as online, the setup was completed successfully. | . Stop everything. Navigate to the source directory and run the following command. docker-compose down . ",
    "url": "/worker/#volumes",
    
    "relUrl": "/worker/#volumes"
  },"33": {
    "doc": "3. Worker Setup",
    "title": "3. Worker Setup",
    "content": " ",
    "url": "/worker/",
    
    "relUrl": "/worker/"
  },"34": {
    "doc": "Developer Guide",
    "title": "DataCROP Maize Workflow Management Engine Developer Guide",
    "content": "The Workflow Management Engine (WME) is the DataCROP Maize application for modeling workflows and running processors. This guide focuses on developer-facing integration points, especially how to package processors and connect them to WME data inputs and outputs. ",
    "url": "/dev-guide/#datacrop-maize-workflow-management-engine-developer-guide",
    
    "relUrl": "/dev-guide/#datacrop-maize-workflow-management-engine-developer-guide"
  },"35": {
    "doc": "Developer Guide",
    "title": "Who this guide is for",
    "content": ". | Developers integrating custom processors into WME workflows. | Teams packaging services with docker-compose.yml and runtime .env configuration. | Engineers wiring processors to WME digital resources for data input and output. | . ",
    "url": "/dev-guide/#who-this-guide-is-for",
    
    "relUrl": "/dev-guide/#who-this-guide-is-for"
  },"36": {
    "doc": "Developer Guide",
    "title": "What you’ll learn",
    "content": ". | What “processor integration” means in WME: a packaging contract plus a data I/O compatibility contract. | How to expose WME-compatible env vars derived from data interface types. | Where to find user-guide context on digital resources and workflow wiring. | . ",
    "url": "/dev-guide/#what-youll-learn",
    
    "relUrl": "/dev-guide/#what-youll-learn"
  },"37": {
    "doc": "Developer Guide",
    "title": "Developer guide map",
    "content": ". | Integrating Custom Processors: packaging requirements and WME-compatible data I/O env vars. | Processor definitions and Model Repository defaults (planned). | Workflow execution and worker deployment integration points (planned). | API and automation hooks (planned). | . ",
    "url": "/dev-guide/#developer-guide-map",
    
    "relUrl": "/dev-guide/#developer-guide-map"
  },"38": {
    "doc": "Developer Guide",
    "title": "Developer Guide",
    "content": " ",
    "url": "/dev-guide/",
    
    "relUrl": "/dev-guide/"
  },"39": {
    "doc": "Integrating Custom Processors",
    "title": "Integrating Custom Processors",
    "content": " ",
    "url": "/integrating-processors/",
    
    "relUrl": "/integrating-processors/"
  },"40": {
    "doc": "Integrating Custom Processors",
    "title": "Overview",
    "content": "A processor is a runnable service in a WME workflow. Integrating one means packaging it so WME can start it and inject data connection settings from user-selected digital resources. This page defines the packaging contract and the WME-compatible env var convention for data I/O. ",
    "url": "/integrating-processors/#overview",
    
    "relUrl": "/integrating-processors/#overview"
  },"41": {
    "doc": "Integrating Custom Processors",
    "title": "Packaging requirements",
    "content": "Folder layout example . my-processor/ ├─ docker-compose.yml ├─ .env.example └─ app/ └─ ... docker-compose.yml best practices . | Read configuration from env vars instead of hardcoding endpoints. | Keep secrets out of git; use .env locally and .env.example for documentation. | Keep the compose file focused on running the service, not provisioning infrastructure. | .env as the runtime contract . Your .env (or .env.example) lists every value a deployer may need to configure at runtime. Treat this as the interface between WME and your container: every WME-injected data input or output should be represented by an env var documented here. You can also define app-specific env vars alongside the data-source ones. WME only auto-injects the data-resource variables when using the editor wiring; the rest remain user-configurable. ",
    "url": "/integrating-processors/#packaging-requirements",
    
    "relUrl": "/integrating-processors/#packaging-requirements"
  },"42": {
    "doc": "Integrating Custom Processors",
    "title": "Digital resources and data interface types",
    "content": "A digital resource is an instantiated data source or sink with parameter values. Each digital resource is based on a data interface type, which defines the required parameter keys (for example, kafka.bootstrap_servers and kafka.topic_id). Users create these in the Warehouse and assign them as workflow inputs and outputs in the Lab. When a processor is assigned a digital resource as its data input or output, WME injects the corresponding env vars at runtime. For more detail, see: . | Creating Data Models (digital resources and interface types) | Creating Workflows (assigning data input/output) | . If you rely on the default interface types seeded by the Model Repository, see Model Repository Setup for the current list and keys. ",
    "url": "/integrating-processors/#digital-resources-and-data-interface-types",
    
    "relUrl": "/integrating-processors/#digital-resources-and-data-interface-types"
  },"43": {
    "doc": "Integrating Custom Processors",
    "title": "WME compatibility env var convention",
    "content": "WME passes digital resource parameters into your container using a consistent env var naming rule: . &lt;INTERFACE&gt;_&lt;KEY&gt;_&lt;DIRECTION&gt; . Normalization rules: . | &lt;INTERFACE&gt; is the uppercase data interface type name (for example, kafka becomes KAFKA). | &lt;KEY&gt; is the uppercase parameter key with non-alphanumerics converted to _ (for example, bootstrap_servers becomes BOOTSTRAP_SERVERS). | &lt;DIRECTION&gt; is INPUT for parameters consumed by your processor and OUTPUT for parameters written by your processor. | . Direction rules: . | Use _INPUT for values read from the selected input digital resource. | Use _OUTPUT for values written to the selected output digital resource. | If your processor can be used for either direction, expose both input and output variants for shared connection fields. | . Note: This env var convention is the WME compatibility interface for processor integrations, even as runtime wiring evolves. Implement it now to stay compatible with the visual editor workflow. ",
    "url": "/integrating-processors/#wme-compatibility-env-var-convention",
    
    "relUrl": "/integrating-processors/#wme-compatibility-env-var-convention"
  },"44": {
    "doc": "Integrating Custom Processors",
    "title": "Examples from default interface types",
    "content": "Kafka input . KAFKA_BOOTSTRAP_SERVERS_INPUT=... KAFKA_TOPIC_ID_INPUT=... Kafka output . KAFKA_BOOTSTRAP_SERVERS_OUTPUT=... KAFKA_TOPIC_ID_OUTPUT=... RabbitMQ input and output . RABBITMQ_HOST_INPUT=... RABBITMQ_PORT_INPUT=... RABBITMQ_USER_INPUT=... RABBITMQ_PASSWORD_INPUT=... RABBITMQ_QUEUE_INPUT=... RABBITMQ_HOST_OUTPUT=... RABBITMQ_PORT_OUTPUT=... RABBITMQ_USER_OUTPUT=... RABBITMQ_PASSWORD_OUTPUT=... RABBITMQ_EXCHANGE_OUTPUT=... RABBITMQ_EXCHANGE_TYPE_OUTPUT=... RABBITMQ_VHOST_OUTPUT=... ",
    "url": "/integrating-processors/#examples-from-default-interface-types",
    
    "relUrl": "/integrating-processors/#examples-from-default-interface-types"
  },"45": {
    "doc": "Integrating Custom Processors",
    "title": "Compatibility checklist",
    "content": ". | Package the processor as a folder with your app, docker-compose.yml, and .env or .env.example. | Ensure docker-compose.yml reads configuration from env vars, not hardcoded endpoints. | Expose WME-compatible env vars using the &lt;INTERFACE&gt;_&lt;KEY&gt;_&lt;DIRECTION&gt; pattern. | Provide both input and output variants when your processor supports either direction. | Document the interface types and keys your processor expects, matching the Model Repository definitions. | . ",
    "url": "/integrating-processors/#compatibility-checklist",
    
    "relUrl": "/integrating-processors/#compatibility-checklist"
  },"46": {
    "doc": "Home",
    "title": "DataCROP™ Information",
    "content": "DataCROP™ (Data Collection Routing &amp; Processing) is a Data collection framework which provides the specifications and relevant implementation to enable a real time data collection, transformation, filtering, and management service to facilitate data consumers (i.e., analytic algorithms). The framework can be applied in IoT environments supporting solutions in various domains (e.g., Industrial, Cybersecurity, etch.). For example, the solution may be used to collect security related data (e.g., network, system, solution proprietary, etch.) from monitored IoT systems and store them to detect patterns of abnormal behaviour by applying simple (i.e., filtering and pre-processing) or more elaborated mechanisms (i.e., AI algorithms). The design of the framework is driven by configurability, extensibility, dynamic setup, stream handling capabilities and Blockchain (Ledger) support. One of the key features of the framework is that it is detached from the underlying infrastructure by employing a specialized data model for modelling the solution’s Data Sources, Processors and Results which facilitates the data interoperability discoverability and configurability of the offered solution. ",
    "url": "/home/#datacrop-information",
    
    "relUrl": "/home/#datacrop-information"
  },"47": {
    "doc": "Home",
    "title": "Demonstrator",
    "content": "A DataCROP™ Farro version demo infrastracture instance, which can be deployed locally, can be found at the farro-demo-deployment-scripts repository. ",
    "url": "/home/#demonstrator",
    
    "relUrl": "/home/#demonstrator"
  },"48": {
    "doc": "Home",
    "title": "Technologies/Framework",
    "content": "DataCROP has been developed and applied in various iterations, within the context of various EU projects, with different dependencies . DataCROP™ Barley (v1.0) Outcome of FAR-EDGE EU Project . | MongoDB | Apache Kafka (Inter-Data Bus) | RabitMQ (Data Input interface) | Kafka Streams (Stream Processing) | NodeJS (Component Implementation) | React (UI) | Hyperledger Fabric (optional Blockchain support) | . DataCROP™ Farro (v2.0) Outcome of Barley version and PROPHESY EU project . | MongoDB | Apache Kafka (Inter-Data Bus communication &amp; input/output interface) | RabbitMQ (Data Input interface) | Algorithms: ** Java Algorithms (Qarma) ** Python Algorithms (RUL feature extraction &amp; Health Index Calculation) ** R Algorithms | NodeJS (Component Implementation) | React (UI) | . DataCROP™ Maize :corn: (V3.0) - (under construction :construction:) Outcome of Farro version and SecureIoT, IoTAC EU projects . | MongoDB | Apache Kafka (Inter-Data Bus) | ELK Stack | Additional to be included as developments evolves | . ",
    "url": "/home/#technologiesframework",
    
    "relUrl": "/home/#technologiesframework"
  },"49": {
    "doc": "Home",
    "title": "Maturity Level / Active years",
    "content": "V1.0 - TRL 6 Designed/Developed and demonstrated under the FAR-EDGE (2016-2019) project . V2.0 - TRL6 Designed/Developed and demonstrated under the H2020 PROPHESY (2017-2020) project. Demonstrated under the QU4LITY (2019-2022) project . V3.0 – TRL4 (Under design/Development) Designed/Developed under the SecureIoT (2018-2021) project, IoTAC (2020-2023), STAR (2020-2023) . ",
    "url": "/home/#maturity-level--active-years",
    
    "relUrl": "/home/#maturity-level--active-years"
  },"50": {
    "doc": "Home",
    "title": "Future/ Interest Steps",
    "content": ". | Intergrade the design/data models/components into one platform/infrastructure independent solution. | Support additional data collection, data processing and data offering services. | Offer an intuitive and user-friendly configuration toolbox (UI). | Offer data visualization mechanisms (UI). | Blockchain (Hyperledger Fabric) integration supporting the configurations and results of the solution. | . ",
    "url": "/home/#future-interest-steps",
    
    "relUrl": "/home/#future-interest-steps"
  },"51": {
    "doc": "Home",
    "title": "Links:",
    "content": ". | Contact DataCROP | Report Issues/Problems | DataCROP Forum | Website (under construction) | DataCROP@DockerHub | Stats@OpenHUB | . ",
    "url": "/home/#links",
    
    "relUrl": "/home/#links"
  },"52": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/home/",
    
    "relUrl": "/home/"
  },"53": {
    "doc": "Index",
    "title": "DataCROP™ Information",
    "content": "DataCROP™ (Data Collection Routing &amp; Processing) is a Data collection framework which provides the specifications and relevant implementation to enable a real time data collection, transformation, filtering, and management service to facilitate data consumers (i.e., analytic algorithms). The framework can be applied in IoT environments supporting solutions in various domains (e.g., Industrial, Cybersecurity, etch.). For example, the solution may be used to collect security related data (e.g., network, system, solution proprietary, etch.) from monitored IoT systems and store them to detect patterns of abnormal behaviour by applying simple (i.e., filtering and pre-processing) or more elaborated mechanisms (i.e., AI algorithms). The design of the framework is driven by configurability, extensibility, dynamic setup, stream handling capabilities and Blockchain (Ledger) support. One of the key features of the framework is that it is detached from the underlying infrastructure by employing a specialized data model for modelling the solution’s Data Sources, Processors and Results which facilitates the data interoperability discoverability and configurability of the offered solution. ",
    "url": "/#datacrop-information",
    
    "relUrl": "/#datacrop-information"
  },"54": {
    "doc": "Index",
    "title": "Demonstrator",
    "content": "A DataCROP™ Farro version demo infrastracture instance, which can be deployed locally, can be found at the farro-demo-deployment-scripts repository. ",
    "url": "/#demonstrator",
    
    "relUrl": "/#demonstrator"
  },"55": {
    "doc": "Index",
    "title": "Technologies/Framework",
    "content": "DataCROP has been developed and applied in various iterations, within the context of various EU projects, with different dependencies . DataCROP™ Barley (v1.0) Outcome of FAR-EDGE EU Project . | MongoDB | Apache Kafka (Inter-Data Bus) | RabitMQ (Data Input interface) | Kafka Streams (Stream Processing) | NodeJS (Component Implementation) | React (UI) | Hyperledger Fabric (optional Blockchain support) | . DataCROP™ Farro (v2.0) Outcome of Barley version and PROPHESY EU project . | MongoDB | Apache Kafka (Inter-Data Bus communication &amp; input/output interface) | RabbitMQ (Data Input interface) | Algorithms: ** Java Algorithms (Qarma) ** Python Algorithms (RUL feature extraction &amp; Health Index Calculation) ** R Algorithms | NodeJS (Component Implementation) | React (UI) | . DataCROP™ Maize :corn: (V3.0) - (under construction :construction:) Outcome of Farro version and SecureIoT, IoTAC EU projects . | MongoDB | Apache Kafka (Inter-Data Bus) | ELK Stack | Additional to be included as developments evolves | . ",
    "url": "/#technologiesframework",
    
    "relUrl": "/#technologiesframework"
  },"56": {
    "doc": "Index",
    "title": "Maturity Level / Active years",
    "content": "V1.0 - TRL 6 Designed/Developed and demonstrated under the FAR-EDGE (2016-2019) project . V2.0 - TRL6 Designed/Developed and demonstrated under the H2020 PROPHESY (2017-2020) project. Demonstrated under the QU4LITY (2019-2022) project . V3.0 – TRL4 (Under design/Development) Designed/Developed under the SecureIoT (2018-2021) project, IoTAC (2020-2023), STAR (2020-2023) . ",
    "url": "/#maturity-level--active-years",
    
    "relUrl": "/#maturity-level--active-years"
  },"57": {
    "doc": "Index",
    "title": "Future/ Interest Steps",
    "content": ". | Intergrade the design/data models/components into one platform/infrastructure independent solution. | Support additional data collection, data processing and data offering services. | Offer an intuitive and user-friendly configuration toolbox (UI). | Offer data visualization mechanisms (UI). | Blockchain (Hyperledger Fabric) integration supporting the configurations and results of the solution. | . ",
    "url": "/#future-interest-steps",
    
    "relUrl": "/#future-interest-steps"
  },"58": {
    "doc": "Index",
    "title": "Links:",
    "content": ". | Contact DataCROP | Report Issues/Problems | DataCROP Forum | Website (under construction :construction:) | DataCROP@DockerHub | Stats@OpenHUB | . ",
    "url": "/#links",
    
    "relUrl": "/#links"
  },"59": {
    "doc": "Index",
    "title": "Index",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"60": {
    "doc": "4. Airflow",
    "title": "Airflow",
    "content": ". For all the workflows we create, two DAGs (Directed Acyclic Graphs) are generated in Airflow: . | Deployment DAG: Responsible for deploying all the processors in the workflow. | Undeployment DAG: Responsible for undeploying all the processors in the workflow. | . This is why there is an Airflow icon in the sidebar, which contains an iframe that takes you directly to the Airflow web page. Additionally, each workflow in the Lab page has an Airflow tab to inspect the corresponding Airflow DAG. ",
    "url": "/airflow-note/#airflow",
    
    "relUrl": "/airflow-note/#airflow"
  },"61": {
    "doc": "4. Airflow",
    "title": "4. Airflow",
    "content": " ",
    "url": "/airflow-note/",
    
    "relUrl": "/airflow-note/"
  },"62": {
    "doc": "2. Creating Data Models",
    "title": "Creating Data Models",
    "content": " ",
    "url": "/creating-data-models/#creating-data-models",
    
    "relUrl": "/creating-data-models/#creating-data-models"
  },"63": {
    "doc": "2. Creating Data Models",
    "title": "Warehouse Tab",
    "content": "My Workflows . Description: This section displays a list of workflows you have created. Features: . | View Workflows: Browse your existing workflows. | Create Workflow: Redirects you to the Lab tab for creating a new workflow. | . My Digital Resources . Description: The Digital Resource entity represents the instantiation of a data source within the system. Purpose: Provides a context and connection interface for accessing and utilizing data. Key Attributes: . | dataInterfaceTypeID: Links to the data interface type (e.g., kafka, elasticsearch, API REST). | dataKindID: (Optional) Links to the type of data the resource manages. | parameterValue: A list of parameter values required by the selected data interface type. | . Functionality: Provides reusable, parameterized connection details (for example Kafka topic configuration or Elasticsearch connection details) that processors can reference as workflow inputs/outputs. My Workers . Description: This section allows you to view and create worker assets. Purpose: Worker assets are responsible for executing processors within workflows. Features: . | Define worker specifications. | Assign workers to processors for deployment. | . My Data Kinds . Description: The Data Kind entity defines the type of data managed in the system. Purpose: Provides metadata about data structure, format, and type. Key Attributes: . | modelType: Specifies the data’s structural type (e.g., simple or complex). | format: Indicates the format (e.g., JSON, XML). | quantityKind: Defines whether the data is quantitative or qualitative. | . Functionality: Facilitates precise processing and analysis of data. ",
    "url": "/creating-data-models/#warehouse-tab",
    
    "relUrl": "/creating-data-models/#warehouse-tab"
  },"64": {
    "doc": "2. Creating Data Models",
    "title": "2. Creating Data Models",
    "content": " ",
    "url": "/creating-data-models/",
    
    "relUrl": "/creating-data-models/"
  },"65": {
    "doc": "3. Creating Workflows",
    "title": "Creating Workflows",
    "content": " ",
    "url": "/creating-workflows/#creating-workflows",
    
    "relUrl": "/creating-workflows/#creating-workflows"
  },"66": {
    "doc": "3. Creating Workflows",
    "title": "Lab Tab",
    "content": "Creating Workflows . Workflow Specifications Page . Overview: The starting point for creating workflows. Workflow Specifications: . | Name | Description | Configuration: Define the DAG (Directed Acyclic Graph) that represents the workflow in Airflow. | . Flow Creator . Overview: An interactive interface for designing workflows using nodes and edges. Features: . | Add Node: Use the “Add Node” button to add processors to your workflow. | View Workflow: Nodes are displayed in a 2D representation and can be connected using edges. | . Processors . Processor Configuration . | Define the Processor Type and it’s parameters. | Choose a Worker for deployment. | Assign Data Input and Data Output using digital resources. | . Note: Before creating a processor in the Flow Creator, ensure that you have already created the data models you plan to use with it. This includes the worker asset and the digital resources that will represent the data input and output you plan to add. Basic Processor Types Supported: . | Apache Kafka: Creates a Kafka cluster (used as infrastructure for streaming workflows). | Logstash Pipeline: Creates a pipeline that transfers data from one digital resource to another (supports a custom logstash_filter). | Kibana Pipeline: Creates a pipeline that takes input from a digital resource and forwards it to Elasticsearch for visualization through the editor. | Context Extraction: Example processing component shipped as a processor definition (your deployment may include different processors). | . Note: The list of available processors is driven by processor definitions stored in the Model Repository. Deployers can add more definitions via config/extra-processors.json and import them during initialization. ",
    "url": "/creating-workflows/#lab-tab",
    
    "relUrl": "/creating-workflows/#lab-tab"
  },"67": {
    "doc": "3. Creating Workflows",
    "title": "3. Creating Workflows",
    "content": " ",
    "url": "/creating-workflows/",
    
    "relUrl": "/creating-workflows/"
  },"68": {
    "doc": "User Guide",
    "title": "DataCROP Maize Workflow Management Engine User Guide",
    "content": " ",
    "url": "/user-guide/#datacrop-maize-workflow-management-engine-user-guide",
    
    "relUrl": "/user-guide/#datacrop-maize-workflow-management-engine-user-guide"
  },"69": {
    "doc": "User Guide",
    "title": "Overview",
    "content": "The Workflow Management Engine application is designed to facilitate efficient creation and management of workflows through four primary tabs in the sidebar: Warehouse, Lab, Airflow, and Kibana. Each tab serves a distinct purpose within the application. | Warehouse: A repository for creating and managing the data models that are used in workflows. | Lab: A dedicated space for designing and configuring workflows, leveraging the data models from the Warehouse. | Airflow: For monitoring the deployment of components. | Kibana: For data visualization. | . ",
    "url": "/user-guide/#overview",
    
    "relUrl": "/user-guide/#overview"
  },"70": {
    "doc": "User Guide",
    "title": "User Guide",
    "content": " ",
    "url": "/user-guide/",
    
    "relUrl": "/user-guide/"
  },"71": {
    "doc": "1. Getting Started",
    "title": "Getting Started",
    "content": "Before using the Workflow Editor, choose a deployment path on the Maize Setup page: either the single-script Maize MVP approach or the manual per-repository setup. Available processors depend on your deployment; administrators can ship predefined processor definitions during deployment (see Model Repository Setup). ",
    "url": "/overview/#getting-started",
    
    "relUrl": "/overview/#getting-started"
  },"72": {
    "doc": "1. Getting Started",
    "title": "Accessing the Workflow Editor",
    "content": "To access the Workflow Editor, you log in using Keycloak. The exact login options depend on how your Keycloak realm is configured (for example, username/password, or an external identity provider such as GitHub if enabled). For the backend API calls used by the editor, the JWT token must include a userId claim, otherwise the backend cannot resolve the current user and requests will fail. ",
    "url": "/overview/#accessing-the-workflow-editor",
    
    "relUrl": "/overview/#accessing-the-workflow-editor"
  },"73": {
    "doc": "1. Getting Started",
    "title": "1. Getting Started",
    "content": " ",
    "url": "/overview/",
    
    "relUrl": "/overview/"
  }
}
