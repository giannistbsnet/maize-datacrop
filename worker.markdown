---
layout: page
title: 3. Worker Setup
parent: Maize Setup
permalink: /worker/
nav_order: 3
---

# DataCROP Maize Processing Engine Worker Deployment

This is a demo deployment instance for the **Maize DataCROP version**. It deploys a Worker responsible for handling tasks within the **DataCROP Workflow Management Engine**. The deployment consists of a single container.

## Overview

The deployment utilizes **Apache Airflow** and **CeleryExecutor** for distributed task execution within the DataCROP system. Below is an explanation of the different components and configurations defined in the `docker-compose.yml` file.

## Airflow Worker Setup

- The `airflow-worker` service is set up using **Airflow's CeleryExecutor** to manage distributed task execution.
- The worker communicates with:
  - **Redis**: Used as the message broker for Celery.
  - **PostgreSQL**: Used as the backend for storing task results.

## Volumes

The following directories are mounted into the Airflow worker container to persist data and provide necessary resources:

- **DAGs**: Task definitions are stored in the `./dags` folder.
- **Logs**: Logs generated by Airflow are stored in the `./logs` folder.
- **Data**: Input and output data for tasks are stored in the `./data` folder.
- **Models**: Model data is stored in the `./models` folder.
- **Plugins**: Airflow plugins can be added via the `./plugins` folder.
- **.env**: The `.env` file is used to handle dynamic environment variables.


### REQUIREMENTS

- [Docker-CE](https://www.docker.com/)


### PREREQUISITES

Before proceeding, ensure that you have followed the setup instructions for the [airflow processing engine](https://github.com/datacrop/maze-processing-engine-airflow).

After completing the setup, follow these steps to configure your environment variables:

1. Navigate to the [`.env` file](.env) and ensure that all necessary environment variables are set correctly for your deployment. Update the file with the correct values for your infrastructure by filling in the following parameters:

    ```plaintext
    # HOST              ||  DC.C
    AIRFLOW_IP=<YOUR_AIRFLOW_IP>
    AIRFLOW_WEB_SECRET_KEY=<YOUR_AIRFLOW_WEB_SECRET_KEY>
    AIRFLOW_FERNET_KEY=<YOUR_AIRFLOW_FERNET_KEY>
    HOST_IP=<YOUR_HOST_IP>

    # WORKER            ||  DC.W
    WORKER_NAME=<YOUR_WORKER_NAME>
    WORKER_SSL_KEY_FILE=<YOUR_WORKER_SSL_KEY_FILE>
    WORKER_SSL_CERT_FILE=<YOUR_WORKER_SSL_CERT_FILE>
    WORKER_SSL_CERT_STORE=<YOUR_WORKER_SSL_CERT_STORE>

    # REDIS             ||  DC.C
    REDIS_TLS_PORT=<YOUR_REDIS_TLS_PORT>
    REDIS_TLS_CERT_FILE=<YOUR_REDIS_TLS_CERT_FILE>
    REDIS_TLS_KEY_FILE=<YOUR_REDIS_TLS_KEY_FILE>
    REDIS_TLS_CA_CERT_FILE=<YOUR_REDIS_TLS_CA_CERT_FILE>
    REDIS_TLS_CLIENT_CERT_FILE=<YOUR_REDIS_TLS_CLIENT_CERT_FILE>
    REDIS_TLS_CLIENT_KEY_FILE=<YOUR_REDIS_TLS_CLIENT_KEY_FILE>
    REDIS_IP=<YOUR_REDIS_IP>

    POSTGRES_IP=<YOUR_POSTGRES_IP>

    # ARTIFACTORY       ||  CONFIG
    ARTIFACTORY_HOST=<YOUR_ARTIFACTORY_HOST>
    ARTIFACTORY_REGISTRY=<YOUR_ARTIFACTORY_REGISTRY>
    ARTIFACTORY_USER=<YOUR_ARTIFACTORY_USER>
    ARTIFACTORY_PSSWD=<YOUR_ARTIFACTORY_PSSWD>

    # Elastic           ||  CONFIG
    ELASTIC_HOST=<YOUR_ELASTIC_HOST>
    ELASTIC_PORT=<YOUR_ELASTIC_PORT>
    INDEX=<YOUR_INDEX>

    # KAFKA             ||  CONFIG
    KAFKA_TOPIC=<YOUR_KAFKA_TOPIC>
    KAFKA_BROKER=<YOUR_KAFKA_BROKER>
    KAFKA_CONSUMER_GROUP=<YOUR_KAFKA_CONSUMER_GROUP>

    # TENSORBOARD       ||  CONFIG
    TB_LOGS_PATH=<YOUR_TB_LOGS_PATH>

    # MODULES LIB
    MODULES_LIB_PATH=<YOUR_MODULES_LIB_PATH>
    ```

   Replace all placeholder values (e.g., `<YOUR_AIRFLOW_IP>`, `<YOUR_WORKER_NAME>`, etc.) with the actual values for your infrastructure.

Once these parameters are correctly set, you can proceed with the deployment.


### Start The Application.

1. Navigate to the source directory containing the `docker-compose.yml` file.
2. Run the following command:

    ```bash
    docker compose up -d
    ```


### Verify that everything is up and running

Wait for the services to start, then run the following commands:

- Check if the container is running (change `worker_name` with the actual name that you specified in the .env file):

    ```bash
    docker ps --filter name=[worker_name] --format "table {% raw %}{{.Image}}{% endraw %}\t{% raw %}{{.Names}}{% endraw %}"
    ```

    You should see the following output:

    ```bash
    IMAGE                                        NAMES
    [worker_name]-airflow-worker                [worker_name]
    ```


### Make Sure Everything Works

1. Open a browser and navigate to the `flower web app` (http://{Your IP}:5555/workers).
2. Enter the credentials provided by your organization for celery.
3. After successful authentication, you will be redirected to the workers page, where the newly created worker should appear in the workers table. If its status is marked as **online**, the setup was completed successfully.


#### Stop everything.

Navigate to the source directory and run the following command.

    docker-compose down